{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zzlSS5vsxSS"
      },
      "source": [
        "# HLCV Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62OCVFrnRZGx"
      },
      "outputs": [],
      "source": [
        "Name = \"Luisa Danalachi\"\n",
        "Matriculation_Number = \"7022909\"\n",
        "\n",
        "Name = \"Victor Martinez Palomares\"\n",
        "Matriculation_Number = \"7021729\"\n",
        "\n",
        "Name = \"Soham Roy\"\n",
        "Matriculation_Number = \"7028704\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6fBKi5dRdVQ",
        "outputId": "929bb8f3-efd3-4cc1-9d43-1d05984eb06e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "mkdir: cannot create directory ‘./datasets’: File exists\n",
            "mkdir: cannot create directory ‘./datasets/lyme_dataset’: File exists\n"
          ]
        }
      ],
      "source": [
        " from google.colab import drive\n",
        " drive.mount('/content/drive')\n",
        "\n",
        " !mkdir ./datasets\n",
        " !mkdir ./datasets/lyme_dataset\n",
        " !cp -r drive/MyDrive/archive.zip ./datasets\n",
        " !unzip -q -o \"./datasets/archive.zip\" -d \"./datasets/lyme_dataset\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !mkdir ./datasets\n",
        "# !mkdir ./datasets/lyme_dataset\n",
        "# !cp -r drive/MyDrive/Colab\\ Notebooks/HLCV_project/datasets/lyme_dataset.zip ./datasets\n",
        "# !unzip -q -o \"./datasets/lyme_dataset.zip\" -d \"./datasets/lyme_dataset\"\n",
        "\n",
        "# '''\n",
        "# !mkdir ./resources\n",
        "\n",
        "# !cp -r drive/MyDrive/Colab\\ Notebooks/HLCV/Exercise_3/resources/fig1.png ./resources\n",
        "# !cp -r drive/MyDrive/Colab\\ Notebooks/HLCV/Exercise_3/resources/fig2.png ./resources\n",
        "\n",
        "# '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV_zGNoMsxSa"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nifGyyAjRiJd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import read_image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch.utils.data as data\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRgAQ3OtKLeE"
      },
      "source": [
        "### Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTnqkXkIKKs8",
        "outputId": "5286b1ba-7109-423a-8c13-c3159c244637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "IMAGE_SIZE = 256\n",
        "EPOCHS = 120\n",
        "BATCH = 4\n",
        "LR = 1e-4\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: %s'%device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJNOEAWNsxSe"
      },
      "source": [
        "# Define data augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vc02dP-sWBLl"
      },
      "outputs": [],
      "source": [
        "data_aug_transforms = []\n",
        "\n",
        "data_aug_transforms.append(transforms.RandomRotation([-90, 90]) ) \n",
        "data_aug_transforms.append( transforms.RandomHorizontalFlip() )\n",
        "data_aug_transforms.append(transforms.ColorJitter(brightness = 0.2)) \n",
        "\n",
        "norm_transforms = [transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "                   transforms.ToTensor(), \n",
        "                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "\n",
        "train_transforms = transforms.Compose(data_aug_transforms + norm_transforms)\n",
        "\n",
        "# Add Compose\n",
        "test_transforms =transforms.Compose(norm_transforms) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkZvV01HsxSg"
      },
      "source": [
        "# Load Lyme DS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_za5Zr7sxSg",
        "outputId": "0ba2ab50-7f14-4a84-ed54-0802fd62d9fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lyme Dermnet data: Dataset ImageFolder\n",
            "    Number of datapoints: 357\n",
            "    Root location: ./datasets/lyme_dataset/RashData/Train/Train_2_Cases\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               RandomRotation(degrees=[-90.0, 90.0], interpolation=nearest, expand=False, fill=0)\n",
            "               RandomHorizontalFlip(p=0.5)\n",
            "               ColorJitter(brightness=[0.8, 1.2], contrast=None, saturation=None, hue=None)\n",
            "               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=None)\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
            "           )\n",
            "Lyme classes: ['Lyme_Negative', 'Lyme_Positive']\n"
          ]
        }
      ],
      "source": [
        "#Load Lyme\n",
        "#lyme_train_data_path = \"./datasets/Lyme/Train/Train_2_Cases\"\n",
        "#lyme_test_data_path = \"./datasets/Lyme/Validation/Validation_2_Cases\"\n",
        "lyme_train_data_path = \"./datasets/lyme_dataset/RashData/Train/Train_2_Cases\"\n",
        "lyme_test_data_path = \"./datasets/lyme_dataset/RashData/Validation/Validation_2_Cases\"\n",
        "# ImageFolder is a generic data loader where the images are arranged in multiple folders\n",
        "\n",
        "#Load TRAIN\n",
        "lyme_train_data = torchvision.datasets.ImageFolder(root=lyme_train_data_path, transform=train_transforms)\n",
        "lyme_train_data_loader = data.DataLoader(lyme_train_data, batch_size=BATCH, shuffle=True)\n",
        "print(\"Lyme Dermnet data:\", lyme_train_data)\n",
        "\n",
        "# Load TEST \n",
        "lyme_test_data = torchvision.datasets.ImageFolder(root=lyme_test_data_path, transform=test_transforms)\n",
        "lyme_test_data_loader = data.DataLoader(lyme_test_data, batch_size=BATCH)\n",
        "\n",
        "list_of_classes=list(map(str, list(lyme_train_data.classes)) )\n",
        "print(\"Lyme classes:\", list_of_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F7lnDEdsxSh"
      },
      "source": [
        "# Train val split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idUiLrfZsxSh"
      },
      "outputs": [],
      "source": [
        "val_split = 0.1\n",
        "\n",
        "num_training = int((1 - val_split) * len(lyme_train_data))\n",
        "num_validation = len(lyme_train_data) - num_training\n",
        "mask = list(range(num_training))\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(lyme_train_data, mask)\n",
        "mask = list(range(num_training, num_training + num_validation))\n",
        "val_dataset = torch.utils.data.Subset(lyme_train_data, mask)\n",
        "\n",
        "# Create DataLoaders\n",
        "lyme_train_data_loader = data.DataLoader(train_dataset, batch_size=BATCH, shuffle=True, drop_last=True)\n",
        "lyme_validation_data_loader = data.DataLoader(val_dataset, batch_size=BATCH, shuffle=True, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWFEognUsxSi"
      },
      "source": [
        "## Check dataloader "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IH42qrWsxSi"
      },
      "outputs": [],
      "source": [
        "# iterator=iter(lyme_validation_data_loader)\n",
        "# inputs, classes = next(iterator)\n",
        "# print(len(inputs)) \n",
        "\n",
        "# plt.imshow(inputs[0].squeeze().permute(2,1,0))\n",
        "# plt.show()\n",
        "# print(\"CLass: \",classes[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUG6PbRxsxSi"
      },
      "source": [
        "# Load Dermnet DS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7jERw52sxSj"
      },
      "outputs": [],
      "source": [
        "#Load Dermnet\n",
        "dermnet_train_data_path = \"./datasets/Dermnet/train\"\n",
        "dermnet_test_data_path = \"./datasets/Dermnet/test\"\n",
        "\n",
        "# ImageFolder is a generic data loader where the images are arranged in multiple folders\n",
        "dermnet_train_data = torchvision.datasets.ImageFolder(root=dermnet_train_data_path, transform=train_transforms)\n",
        "dermnet_train_data_loader = data.DataLoader(dermnet_train_data, batch_size=BATCH, shuffle=True)\n",
        "print(\"Details Dermnet data:\", dermnet_train_data)\n",
        "\n",
        "# Load test \n",
        "dermnet_test_data = torchvision.datasets.ImageFolder(root=dermnet_test_data_path, transform=test_transforms)\n",
        "dermnet_test_data_loader = data.DataLoader(dermnet_train_data, batch_size=BATCH)\n",
        "\n",
        "# Load list of classes\n",
        "list_of_classes=list(map(str, list(dermnet_train_data.classes)) )\n",
        "print(\"Dermnet classes\", list_of_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTZjH2cbsxSj"
      },
      "source": [
        "# Load HAM DS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDSkaDDHsxSj"
      },
      "outputs": [],
      "source": [
        "ham_df = pd.read_csv('./datasets/HAM10000/HAM10000_metadata.csv')\n",
        "ham_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q98hDnHzsxSj"
      },
      "source": [
        "## Extract images based on their label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "oep-Tu00sxSk"
      },
      "outputs": [],
      "source": [
        "# Take labels\n",
        "name_labels = ham_df[\"dx\"].unique()\n",
        "\n",
        "# Create folders with the label name and add the corresponding images -> Must for using ImageFolder!\n",
        "\n",
        "for label in range(len(name_labels)):\n",
        "    # create folder for each label\n",
        "    label_folder_path = \"./datasets/HAM10000/train/\" + str(name_labels[label])\n",
        "    \n",
        "    # check is path exists if not create folder\n",
        "    if not os.path.exists(label_folder_path):\n",
        "        os.mkdir('./datasets/HAM10000/train/' + name_labels[label] + \"/\" )\n",
        "    \n",
        "    # take the image id corresponding to label\n",
        "    image_names =  ham_df[ham_df['dx'] == name_labels[label]]['image_id']\n",
        "    \n",
        "    # iterate through all image names \n",
        "    for image in image_names:\n",
        "        # create the path for image: either part 1 or part 2\n",
        "        path_folder_1 = \"./datasets/HAM10000/HAM10000_images_part_1/\" + image + \".jpg\"\n",
        "        path_folder_2 = \"./datasets/HAM10000/HAM10000_images_part_2/\" + image + \".jpg\"\n",
        "        \n",
        "        # find where is the image and copy it into the label folder\n",
        "        if os.path.exists(path_folder_1):\n",
        "            shutil.copyfile(path_folder_1, './datasets/HAM10000/train/' + name_labels[label] + \"/\" + image + \".jpg\")\n",
        "        else:\n",
        "            shutil.copyfile(path_folder_2, './datasets/HAM10000/train/' + name_labels[label] + \"/\" + image + \".jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwUgF0fNsxSk"
      },
      "source": [
        "## Load data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tq0ClQuysxSk"
      },
      "outputs": [],
      "source": [
        "#Load Dermnet\n",
        "HAM_train_data_path = \"./datasets/HAM10000/train\"\n",
        "\n",
        "# ImageFolder is a generic data loader where the images are arranged in multiple folders\n",
        "ham_train_data = torchvision.datasets.ImageFolder(root=HAM_train_data_path, transform=train_transforms)\n",
        "ham_train_data_loader = data.DataLoader(ham_train_data, batch_size=BATCH, shuffle=True)\n",
        "print(\"HAM1000 Dermnet data:\", ham_train_data)\n",
        "\n",
        "# # Load list of classes\n",
        "list_of_classes=list(map(str, list(ham_train_data.classes)) )\n",
        "print(\"HAM1000 classes\", list_of_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSDvF5JPQbDS"
      },
      "source": [
        "### Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ct4o47mpQeMc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "eb96647fd6cb4148820352c882d10a64",
            "14ec033041024b8891e881fdb85d0ae8",
            "b8027a74060c4b20a3300f7ada44763e",
            "b05a5fa7604341eaa703bc6b6cda0b91",
            "350c3cc38fc44414bece91d1acba25f0",
            "49d6a16ea5f4405e8d52b946ab53aac2",
            "385928c486184664b5e24fea9295abf3",
            "ad734b5d64f94ac5ac9f127db7378587",
            "aec230cbe9db44c0b99acd03c3c98cd2",
            "b08fc6dab2fc409189b50e545dc5e50c",
            "ab4a8cfa4a7d4907b3935af9d68ea419"
          ]
        },
        "outputId": "b9222aeb-3e70-4e32-9fed-45e5c4189a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb96647fd6cb4148820352c882d10a64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet50(\n",
            "  (net): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Sequential(\n",
            "      (0): Flatten(start_dim=1, end_dim=-1)\n",
            "      (1): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): ReLU()\n",
            "      (4): Linear(in_features=512, out_features=256, bias=True)\n",
            "      (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (6): ReLU()\n",
            "      (7): Linear(in_features=256, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Params to learn:\n",
            "\t net.conv1.weight\n",
            "\t net.bn1.weight\n",
            "\t net.bn1.bias\n",
            "\t net.layer1.0.conv1.weight\n",
            "\t net.layer1.0.bn1.weight\n",
            "\t net.layer1.0.bn1.bias\n",
            "\t net.layer1.0.conv2.weight\n",
            "\t net.layer1.0.bn2.weight\n",
            "\t net.layer1.0.bn2.bias\n",
            "\t net.layer1.0.conv3.weight\n",
            "\t net.layer1.0.bn3.weight\n",
            "\t net.layer1.0.bn3.bias\n",
            "\t net.layer1.0.downsample.0.weight\n",
            "\t net.layer1.0.downsample.1.weight\n",
            "\t net.layer1.0.downsample.1.bias\n",
            "\t net.layer1.1.conv1.weight\n",
            "\t net.layer1.1.bn1.weight\n",
            "\t net.layer1.1.bn1.bias\n",
            "\t net.layer1.1.conv2.weight\n",
            "\t net.layer1.1.bn2.weight\n",
            "\t net.layer1.1.bn2.bias\n",
            "\t net.layer1.1.conv3.weight\n",
            "\t net.layer1.1.bn3.weight\n",
            "\t net.layer1.1.bn3.bias\n",
            "\t net.layer1.2.conv1.weight\n",
            "\t net.layer1.2.bn1.weight\n",
            "\t net.layer1.2.bn1.bias\n",
            "\t net.layer1.2.conv2.weight\n",
            "\t net.layer1.2.bn2.weight\n",
            "\t net.layer1.2.bn2.bias\n",
            "\t net.layer1.2.conv3.weight\n",
            "\t net.layer1.2.bn3.weight\n",
            "\t net.layer1.2.bn3.bias\n",
            "\t net.layer2.0.conv1.weight\n",
            "\t net.layer2.0.bn1.weight\n",
            "\t net.layer2.0.bn1.bias\n",
            "\t net.layer2.0.conv2.weight\n",
            "\t net.layer2.0.bn2.weight\n",
            "\t net.layer2.0.bn2.bias\n",
            "\t net.layer2.0.conv3.weight\n",
            "\t net.layer2.0.bn3.weight\n",
            "\t net.layer2.0.bn3.bias\n",
            "\t net.layer2.0.downsample.0.weight\n",
            "\t net.layer2.0.downsample.1.weight\n",
            "\t net.layer2.0.downsample.1.bias\n",
            "\t net.layer2.1.conv1.weight\n",
            "\t net.layer2.1.bn1.weight\n",
            "\t net.layer2.1.bn1.bias\n",
            "\t net.layer2.1.conv2.weight\n",
            "\t net.layer2.1.bn2.weight\n",
            "\t net.layer2.1.bn2.bias\n",
            "\t net.layer2.1.conv3.weight\n",
            "\t net.layer2.1.bn3.weight\n",
            "\t net.layer2.1.bn3.bias\n",
            "\t net.layer2.2.conv1.weight\n",
            "\t net.layer2.2.bn1.weight\n",
            "\t net.layer2.2.bn1.bias\n",
            "\t net.layer2.2.conv2.weight\n",
            "\t net.layer2.2.bn2.weight\n",
            "\t net.layer2.2.bn2.bias\n",
            "\t net.layer2.2.conv3.weight\n",
            "\t net.layer2.2.bn3.weight\n",
            "\t net.layer2.2.bn3.bias\n",
            "\t net.layer2.3.conv1.weight\n",
            "\t net.layer2.3.bn1.weight\n",
            "\t net.layer2.3.bn1.bias\n",
            "\t net.layer2.3.conv2.weight\n",
            "\t net.layer2.3.bn2.weight\n",
            "\t net.layer2.3.bn2.bias\n",
            "\t net.layer2.3.conv3.weight\n",
            "\t net.layer2.3.bn3.weight\n",
            "\t net.layer2.3.bn3.bias\n",
            "\t net.layer3.0.conv1.weight\n",
            "\t net.layer3.0.bn1.weight\n",
            "\t net.layer3.0.bn1.bias\n",
            "\t net.layer3.0.conv2.weight\n",
            "\t net.layer3.0.bn2.weight\n",
            "\t net.layer3.0.bn2.bias\n",
            "\t net.layer3.0.conv3.weight\n",
            "\t net.layer3.0.bn3.weight\n",
            "\t net.layer3.0.bn3.bias\n",
            "\t net.layer3.0.downsample.0.weight\n",
            "\t net.layer3.0.downsample.1.weight\n",
            "\t net.layer3.0.downsample.1.bias\n",
            "\t net.layer3.1.conv1.weight\n",
            "\t net.layer3.1.bn1.weight\n",
            "\t net.layer3.1.bn1.bias\n",
            "\t net.layer3.1.conv2.weight\n",
            "\t net.layer3.1.bn2.weight\n",
            "\t net.layer3.1.bn2.bias\n",
            "\t net.layer3.1.conv3.weight\n",
            "\t net.layer3.1.bn3.weight\n",
            "\t net.layer3.1.bn3.bias\n",
            "\t net.layer3.2.conv1.weight\n",
            "\t net.layer3.2.bn1.weight\n",
            "\t net.layer3.2.bn1.bias\n",
            "\t net.layer3.2.conv2.weight\n",
            "\t net.layer3.2.bn2.weight\n",
            "\t net.layer3.2.bn2.bias\n",
            "\t net.layer3.2.conv3.weight\n",
            "\t net.layer3.2.bn3.weight\n",
            "\t net.layer3.2.bn3.bias\n",
            "\t net.layer3.3.conv1.weight\n",
            "\t net.layer3.3.bn1.weight\n",
            "\t net.layer3.3.bn1.bias\n",
            "\t net.layer3.3.conv2.weight\n",
            "\t net.layer3.3.bn2.weight\n",
            "\t net.layer3.3.bn2.bias\n",
            "\t net.layer3.3.conv3.weight\n",
            "\t net.layer3.3.bn3.weight\n",
            "\t net.layer3.3.bn3.bias\n",
            "\t net.layer3.4.conv1.weight\n",
            "\t net.layer3.4.bn1.weight\n",
            "\t net.layer3.4.bn1.bias\n",
            "\t net.layer3.4.conv2.weight\n",
            "\t net.layer3.4.bn2.weight\n",
            "\t net.layer3.4.bn2.bias\n",
            "\t net.layer3.4.conv3.weight\n",
            "\t net.layer3.4.bn3.weight\n",
            "\t net.layer3.4.bn3.bias\n",
            "\t net.layer3.5.conv1.weight\n",
            "\t net.layer3.5.bn1.weight\n",
            "\t net.layer3.5.bn1.bias\n",
            "\t net.layer3.5.conv2.weight\n",
            "\t net.layer3.5.bn2.weight\n",
            "\t net.layer3.5.bn2.bias\n",
            "\t net.layer3.5.conv3.weight\n",
            "\t net.layer3.5.bn3.weight\n",
            "\t net.layer3.5.bn3.bias\n",
            "\t net.layer4.0.conv1.weight\n",
            "\t net.layer4.0.bn1.weight\n",
            "\t net.layer4.0.bn1.bias\n",
            "\t net.layer4.0.conv2.weight\n",
            "\t net.layer4.0.bn2.weight\n",
            "\t net.layer4.0.bn2.bias\n",
            "\t net.layer4.0.conv3.weight\n",
            "\t net.layer4.0.bn3.weight\n",
            "\t net.layer4.0.bn3.bias\n",
            "\t net.layer4.0.downsample.0.weight\n",
            "\t net.layer4.0.downsample.1.weight\n",
            "\t net.layer4.0.downsample.1.bias\n",
            "\t net.layer4.1.conv1.weight\n",
            "\t net.layer4.1.bn1.weight\n",
            "\t net.layer4.1.bn1.bias\n",
            "\t net.layer4.1.conv2.weight\n",
            "\t net.layer4.1.bn2.weight\n",
            "\t net.layer4.1.bn2.bias\n",
            "\t net.layer4.1.conv3.weight\n",
            "\t net.layer4.1.bn3.weight\n",
            "\t net.layer4.1.bn3.bias\n",
            "\t net.layer4.2.conv1.weight\n",
            "\t net.layer4.2.bn1.weight\n",
            "\t net.layer4.2.bn1.bias\n",
            "\t net.layer4.2.conv2.weight\n",
            "\t net.layer4.2.bn2.weight\n",
            "\t net.layer4.2.bn2.bias\n",
            "\t net.layer4.2.conv3.weight\n",
            "\t net.layer4.2.bn3.weight\n",
            "\t net.layer4.2.bn3.bias\n",
            "\t net.fc.1.weight\n",
            "\t net.fc.1.bias\n",
            "\t net.fc.2.weight\n",
            "\t net.fc.2.bias\n",
            "\t net.fc.4.weight\n",
            "\t net.fc.4.bias\n",
            "\t net.fc.5.weight\n",
            "\t net.fc.5.bias\n",
            "\t net.fc.7.weight\n",
            "\t net.fc.7.bias\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet50(\n",
              "  (net): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Sequential(\n",
              "      (0): Flatten(start_dim=1, end_dim=-1)\n",
              "      (1): Linear(in_features=2048, out_features=512, bias=True)\n",
              "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (3): ReLU()\n",
              "      (4): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): ReLU()\n",
              "      (7): Linear(in_features=256, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from torchvision import models\n",
        "\n",
        "layer_config= [2048, 512, 256]\n",
        "num_classes = 1\n",
        "num_epochs = 30\n",
        "batch_size = 200\n",
        "learning_rate = 1e-5\n",
        "learning_rate_decay = 0.99\n",
        "\n",
        "class ResNet50(nn.Module):\n",
        "    \"\"\"Create ResNet 50 model pretrained with ImageNet\"\"\"\n",
        "    def __init__(self, n_class, fine_tune, pretrained=True):\n",
        "        super(ResNet50, self).__init__()\n",
        "        self.net = models.resnet50(pretrained=pretrained)\n",
        "        \n",
        "        # add new classifier layers\n",
        "        self.net.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(layer_config[0], layer_config[1]),\n",
        "            nn.BatchNorm1d(layer_config[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(layer_config[1], layer_config[2]),\n",
        "            nn.BatchNorm1d(layer_config[2]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(layer_config[2], n_class)\n",
        "        )       \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        return out.view(-1, 1).squeeze(1).type(torch.FloatTensor) \n",
        "\n",
        "# Initialize the model for this run\n",
        "fine_tune = True\n",
        "pretrained = True\n",
        "model= ResNet50(num_classes, fine_tune, pretrained)\n",
        "print(model)\n",
        "\n",
        "\n",
        "print(\"Params to learn:\")\n",
        "if fine_tune:\n",
        "    params_to_update = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    params_to_update = model.parameters()\n",
        "    for name,param in model.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PAbVYr5sxSl"
      },
      "source": [
        "# Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-FabsTwYqzA"
      },
      "outputs": [],
      "source": [
        "CUDA_LAUNCH_BLOCKING=\"1\"\n",
        "\n",
        "\n",
        "def update_lr(optimizer, lr):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "# store best model and acc\n",
        "best_model_name = 'bestmodel_resnet50_imagenet.ckpt'\n",
        "best_model = None\n",
        "best_val_acc = 0.\n",
        "\n",
        "\n",
        "loss_history = []\n",
        "val_loss_history = []\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(params_to_update, lr=learning_rate)\n",
        "# Train the model\n",
        "lr = learning_rate\n",
        "total_step = len(lyme_train_data_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    correct = 0\n",
        "    total = 0 \n",
        "    for i, (images, labels) in enumerate(lyme_train_data_loader):\n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images).to(device)\n",
        "        predicted = torch.where(torch.sigmoid(outputs.data) > 0.5, 1, 0)\n",
        "        loss = criterion(outputs, labels.float()) # labels are stored as float need cast to int\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    print('Train accuracy is: {} %'.format(100 * correct / total))\n",
        "    train_acc_history.append(100 * correct / total)\n",
        "    loss_history.append(loss.item())\n",
        "    \n",
        "    # Code to update the lr\n",
        "    lr *= learning_rate_decay\n",
        "    update_lr(optimizer, lr)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in lyme_validation_data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(images).to(device)\n",
        "            predicted = torch.where(torch.sigmoid(outputs.data) > 0.5, 1, 0)\n",
        "            \n",
        "            loss = criterion(outputs, labels.float())\n",
        "            \n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "        val_accuracy = 100 * correct / total\n",
        "        if val_accuracy > best_val_acc:\n",
        "            best_val_acc = val_accuracy\n",
        "            best_model = model\n",
        "            print(\"New best validation accuracy: {} %\".format(best_val_acc))\n",
        "\n",
        "\n",
        "        print('Validataion accuracy is: {} %'.format(100 * correct / total))\n",
        "        val_acc_history.append(val_accuracy)\n",
        "        val_loss_history.append(loss.item())\n",
        "  \n",
        "plt.plot(train_acc_history)\n",
        "plt.plot(val_acc_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy history')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# plot the loss history\n",
        "plt.plot(loss_history)\n",
        "plt.plot(val_loss_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss history')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0haxQObpsxSm"
      },
      "source": [
        "# Siamese Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J26LD_0CsxSm"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "\n",
        "# #Load Lyme\n",
        "lyme_train_data_path = \"./datasets/lyme_dataset/RashData/Train/Train_2_Cases\"\n",
        "lyme_test_data_path = \"./datasets/lyme_dataset/RashData/Validation/Validation_2_Cases\"\n",
        "\n",
        "#Load train dataset\n",
        "lyme_train_data = torchvision.datasets.ImageFolder(root=lyme_train_data_path)\n",
        "lyme_test_data = torchvision.datasets.ImageFolder(root=lyme_test_data_path)\n",
        "\n",
        "class LymeDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Src: https://stackoverflow.com/questions/65112063/ive-2-folders-one-image-in-1-folder-and-another-in-another-folder-i-have-to-co\n",
        "    \n",
        "    Note: Code rewritten.\n",
        "    \"\"\"\n",
        "    def __init__(self,lyme_train_data, test):\n",
        "        self.lyme_train_data = lyme_train_data  \n",
        "        self.test = test\n",
        "        \n",
        "        self.data_aug_transforms = [\n",
        "            transforms.RandomRotation([-90, 90]),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ColorJitter(brightness = 0.3)\n",
        "        ]\n",
        "\n",
        "        self.test_transforms = [\n",
        "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "            transforms.ToTensor()\n",
        "        ]\n",
        "\n",
        "        self.train_transform = transforms.Compose(self.data_aug_transforms + self.test_transforms)\n",
        "        \n",
        "        if self.test:\n",
        "            self.transform = self.test_transforms\n",
        "        else:\n",
        "            self.transform =  self.train_transform\n",
        "            \n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        random_image = random.sample(list(lyme_train_data.imgs), 1)\n",
        "        choose_class = numpy.random.randint(0,2) # 0 or 1\n",
        "        \n",
        "        if choose_class:\n",
        "            while True:\n",
        "                new_random_image =random.sample(list(lyme_train_data.imgs), 1)\n",
        "                if random_image[0][1]==new_random_image[0][1]:\n",
        "                    break\n",
        "        else:\n",
        "            while True:\n",
        "                new_random_image = random.sample(list(lyme_train_data.imgs), 1)\n",
        "                if random_image[0][1] != new_random_image[0][1]:\n",
        "                    break\n",
        "\n",
        "        label_image_one = random_image[0][1]\n",
        "        label_image_two = new_random_image[0][1]\n",
        "        \n",
        "        pair_image_one = Image.open(random_image[0][0]).convert(\"RGB\")\n",
        "        pair_image_two = Image.open(new_random_image[0][0]).convert(\"RGB\")\n",
        "        \n",
        "        pair_image_one = self.transform(pair_image_one)\n",
        "        pair_image_two = self.transform(pair_image_two)\n",
        "        # Label 1 = different\n",
        "        # Label 0 = same class\n",
        "        get_pair_label = torch.from_numpy(np.array([label_image_one != label_image_two],dtype=np.float32))\n",
        "        \n",
        "        return [pair_image_one, pair_image_two], get_pair_label, [label_image_one, label_image_two]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.lyme_train_data.imgs) * 2 # we can decide the nr pairs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LymetestDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Src: https://stackoverflow.com/questions/65112063/ive-2-folders-one-image-in-1-folder-and-another-in-another-folder-i-have-to-co\n",
        "    \n",
        "    Note: Code rewritten.\n",
        "    \"\"\"\n",
        "    def __init__(self, test):\n",
        "        self.test = test\n",
        "        \n",
        "\n",
        "        self.test_transforms = [\n",
        "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "            transforms.ToTensor()\n",
        "        ]\n",
        "\n",
        "        self.transform = transforms.Compose(self.test_transforms)\n",
        "\n",
        "            \n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        random_image = random.sample(list(lyme_test_data.imgs), 1)\n",
        "        #choose_class = numpy.random.randint(0,2) # 0 or 1\n",
        "\n",
        "        label_image_one = random_image[0][1]\n",
        "        \n",
        "        pair_image_one = Image.open(random_image[0][0]).convert(\"RGB\")\n",
        "        \n",
        "        pair_image_one = self.transform(pair_image_one)\n",
        "        # Label 1 = different\n",
        "        # Label 0 = same class\n",
        "        \n",
        "        return pair_image_one, label_image_one\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.test.imgs)  # we can decide the nr pairs"
      ],
      "metadata": {
        "id": "9DTw-EmMRs1q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDibeS5zsxSn"
      },
      "source": [
        "# Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "TgcIthr9sxSn",
        "outputId": "34990d45-8bc6-46e1-f3ae-a328bc86fea1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for i, data in enumerate(lyme_train_data_loader):\\n    [pair_image_one, pair_image_two], get_pair_label, [label_image_one, label_image_two] = data\\n    print(\"Pair label:\", get_pair_label[0])\\n    fig, axs = plt.subplots(1, 2, figsize = (10, 10))\\n    axs[0].imshow(pair_image_one[0].squeeze().permute(1, 2, 0))\\n    axs[1].imshow(pair_image_two[0].squeeze().permute(1, 2, 0))\\n    plt.show()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "lyme_dataset = LymeDataset(lyme_train_data, test = False)\n",
        "lyme_train_data_loader = torch.utils.data.DataLoader(lyme_dataset, batch_size=BATCH)\n",
        "\n",
        "lyme_test_dataset = LymetestDataset(lyme_test_data)\n",
        "lyme_test_data_loader = torch.utils.data.DataLoader(lyme_test_dataset, batch_size=BATCH)\n",
        "\n",
        "# split train/ val\n",
        "val_split = 0.1\n",
        "\n",
        "num_training = int((1 - val_split) * len(lyme_dataset))\n",
        "num_validation = len(lyme_dataset) - num_training\n",
        "mask = list(range(num_training))\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(lyme_dataset, mask)\n",
        "mask = list(range(num_training, num_training + num_validation))\n",
        "val_dataset = torch.utils.data.Subset(lyme_dataset, mask)\n",
        "# Create DataLoaders\n",
        "lyme_train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH, shuffle=True)\n",
        "lyme_validation_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH, shuffle=True)\n",
        "\n",
        "'''for i, data in enumerate(lyme_train_data_loader):\n",
        "    [pair_image_one, pair_image_two], get_pair_label, [label_image_one, label_image_two] = data\n",
        "    print(\"Pair label:\", get_pair_label[0])\n",
        "    fig, axs = plt.subplots(1, 2, figsize = (10, 10))\n",
        "    axs[0].imshow(pair_image_one[0].squeeze().permute(1, 2, 0))\n",
        "    axs[1].imshow(pair_image_two[0].squeeze().permute(1, 2, 0))\n",
        "    plt.show()'''\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5oZv9wzlsxSn"
      },
      "outputs": [],
      "source": [
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Source: https://gist.github.com/dimartinot/80abaabaea9a6ef3d9ab0ab199927ee4#file-contrastive_loss-py\n",
        "    TODO: rewrite\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, dist, label):\n",
        "\n",
        "        loss = torch.mean(1/2*(1-label) * torch.pow(dist, 2) +\n",
        "                                      1/2*(label) * torch.pow(torch.clamp(self.margin - dist, min=0.0), 2))\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "scrolled": true,
        "id": "p60re6_PsxSn"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "\n",
        "layer_config= [2048, 512, 256]\n",
        "num_classes = 1\n",
        "num_epochs = 50\n",
        "batch_size = 200\n",
        "learning_rate = 1e-5\n",
        "learning_rate_decay = 0.99\n",
        "\n",
        "class ResNet50(nn.Module):\n",
        "    \"\"\"Create ResNet 50 model pretrained with ImageNet\"\"\"\n",
        "    def __init__(self, n_class, fine_tune, pretrained=True):\n",
        "        super(ResNet50, self).__init__()\n",
        "        self.net = models.resnet50(pretrained=pretrained)\n",
        "        # add new classifier layers\n",
        "        self.net.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(layer_config[0], layer_config[1]),\n",
        "            nn.BatchNorm1d(layer_config[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(layer_config[1], layer_config[2]),\n",
        "            nn.BatchNorm1d(layer_config[2]),\n",
        "            nn.ReLU(),\n",
        "        )       \n",
        "        self.final_class =  nn.Linear(layer_config[2], n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        net_features = self.net(x)\n",
        "        out = self.final_class(net_features)\n",
        "        return net_features, out.view(-1, 1).squeeze(1).type(torch.FloatTensor) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZR3LLwAIsxSo",
        "outputId": "46751d85-c2a1-4f45-fbce-37514020251d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Step [10/161], Loss: 1.6035\n",
            "Epoch [1/50], Step [20/161], Loss: 1.6405\n",
            "Epoch [1/50], Step [30/161], Loss: 0.9020\n",
            "Epoch [1/50], Step [40/161], Loss: 1.2206\n",
            "Epoch [1/50], Step [50/161], Loss: 1.1496\n",
            "Epoch [1/50], Step [60/161], Loss: 1.2616\n",
            "Epoch [1/50], Step [70/161], Loss: 1.0538\n",
            "Epoch [1/50], Step [80/161], Loss: 1.1392\n",
            "Epoch [1/50], Step [90/161], Loss: 0.7518\n",
            "Epoch [1/50], Step [100/161], Loss: 1.1304\n",
            "Epoch [1/50], Step [110/161], Loss: 1.3861\n",
            "Epoch [1/50], Step [120/161], Loss: 1.2996\n",
            "Epoch [1/50], Step [130/161], Loss: 0.9368\n",
            "Epoch [1/50], Step [140/161], Loss: 0.9553\n",
            "Epoch [1/50], Step [150/161], Loss: 1.5667\n",
            "Epoch [1/50], Step [160/161], Loss: 1.1276\n",
            "Train accuracy is: 58.25545171339564 %\n",
            "validation accuracy is: 70.83333333333333 %\n",
            "Epoch [2/50], Step [10/161], Loss: 1.2139\n",
            "Epoch [2/50], Step [20/161], Loss: 1.4305\n",
            "Epoch [2/50], Step [30/161], Loss: 1.3005\n",
            "Epoch [2/50], Step [40/161], Loss: 1.5120\n",
            "Epoch [2/50], Step [50/161], Loss: 1.6253\n",
            "Epoch [2/50], Step [60/161], Loss: 1.3670\n",
            "Epoch [2/50], Step [70/161], Loss: 1.2863\n",
            "Epoch [2/50], Step [80/161], Loss: 1.3334\n",
            "Epoch [2/50], Step [90/161], Loss: 1.4240\n",
            "Epoch [2/50], Step [100/161], Loss: 0.9579\n",
            "Epoch [2/50], Step [110/161], Loss: 1.2351\n",
            "Epoch [2/50], Step [120/161], Loss: 1.3980\n",
            "Epoch [2/50], Step [130/161], Loss: 1.0269\n",
            "Epoch [2/50], Step [140/161], Loss: 1.4564\n",
            "Epoch [2/50], Step [150/161], Loss: 0.5347\n",
            "Epoch [2/50], Step [160/161], Loss: 1.0599\n",
            "Train accuracy is: 70.01557632398755 %\n",
            "validation accuracy is: 70.83333333333333 %\n",
            "Epoch [3/50], Step [10/161], Loss: 0.9406\n",
            "Epoch [3/50], Step [20/161], Loss: 1.1503\n",
            "Epoch [3/50], Step [30/161], Loss: 0.9106\n",
            "Epoch [3/50], Step [40/161], Loss: 0.8871\n",
            "Epoch [3/50], Step [50/161], Loss: 1.1422\n",
            "Epoch [3/50], Step [60/161], Loss: 1.1225\n",
            "Epoch [3/50], Step [70/161], Loss: 0.7438\n",
            "Epoch [3/50], Step [80/161], Loss: 1.1436\n",
            "Epoch [3/50], Step [90/161], Loss: 1.0989\n",
            "Epoch [3/50], Step [100/161], Loss: 1.2483\n",
            "Epoch [3/50], Step [110/161], Loss: 1.3096\n",
            "Epoch [3/50], Step [120/161], Loss: 0.8066\n",
            "Epoch [3/50], Step [130/161], Loss: 0.8905\n",
            "Epoch [3/50], Step [140/161], Loss: 1.0828\n",
            "Epoch [3/50], Step [150/161], Loss: 1.4606\n",
            "Epoch [3/50], Step [160/161], Loss: 0.9201\n",
            "Train accuracy is: 74.61059190031153 %\n",
            "validation accuracy is: 82.63888888888889 %\n",
            "Epoch [4/50], Step [10/161], Loss: 1.2665\n",
            "Epoch [4/50], Step [20/161], Loss: 1.0674\n",
            "Epoch [4/50], Step [30/161], Loss: 1.1754\n",
            "Epoch [4/50], Step [40/161], Loss: 1.1471\n",
            "Epoch [4/50], Step [50/161], Loss: 0.8600\n",
            "Epoch [4/50], Step [60/161], Loss: 1.0800\n",
            "Epoch [4/50], Step [70/161], Loss: 0.7770\n",
            "Epoch [4/50], Step [80/161], Loss: 1.3365\n",
            "Epoch [4/50], Step [90/161], Loss: 0.4053\n",
            "Epoch [4/50], Step [100/161], Loss: 1.0283\n",
            "Epoch [4/50], Step [110/161], Loss: 0.6897\n",
            "Epoch [4/50], Step [120/161], Loss: 1.3437\n",
            "Epoch [4/50], Step [130/161], Loss: 1.0051\n",
            "Epoch [4/50], Step [140/161], Loss: 1.6182\n",
            "Epoch [4/50], Step [150/161], Loss: 1.0591\n",
            "Epoch [4/50], Step [160/161], Loss: 1.2030\n",
            "Train accuracy is: 79.2834890965732 %\n",
            "validation accuracy is: 89.58333333333333 %\n",
            "Epoch [5/50], Step [10/161], Loss: 0.7892\n",
            "Epoch [5/50], Step [20/161], Loss: 0.5920\n",
            "Epoch [5/50], Step [30/161], Loss: 1.1731\n",
            "Epoch [5/50], Step [40/161], Loss: 1.2518\n",
            "Epoch [5/50], Step [50/161], Loss: 0.9798\n",
            "Epoch [5/50], Step [60/161], Loss: 1.1144\n",
            "Epoch [5/50], Step [70/161], Loss: 0.8871\n",
            "Epoch [5/50], Step [80/161], Loss: 0.7936\n",
            "Epoch [5/50], Step [90/161], Loss: 0.9909\n",
            "Epoch [5/50], Step [100/161], Loss: 0.9708\n",
            "Epoch [5/50], Step [110/161], Loss: 1.3587\n",
            "Epoch [5/50], Step [120/161], Loss: 0.8942\n",
            "Epoch [5/50], Step [130/161], Loss: 0.9243\n",
            "Epoch [5/50], Step [140/161], Loss: 1.1413\n",
            "Epoch [5/50], Step [150/161], Loss: 0.9061\n",
            "Epoch [5/50], Step [160/161], Loss: 1.0021\n",
            "Train accuracy is: 79.3613707165109 %\n",
            "validation accuracy is: 87.5 %\n",
            "Epoch [6/50], Step [10/161], Loss: 1.0843\n",
            "Epoch [6/50], Step [20/161], Loss: 0.8560\n",
            "Epoch [6/50], Step [30/161], Loss: 0.4003\n",
            "Epoch [6/50], Step [40/161], Loss: 0.8439\n",
            "Epoch [6/50], Step [50/161], Loss: 0.7701\n",
            "Epoch [6/50], Step [60/161], Loss: 1.0364\n",
            "Epoch [6/50], Step [70/161], Loss: 1.3171\n",
            "Epoch [6/50], Step [80/161], Loss: 0.3071\n",
            "Epoch [6/50], Step [90/161], Loss: 0.8186\n",
            "Epoch [6/50], Step [100/161], Loss: 0.9281\n",
            "Epoch [6/50], Step [110/161], Loss: 0.9598\n",
            "Epoch [6/50], Step [120/161], Loss: 1.1289\n",
            "Epoch [6/50], Step [130/161], Loss: 1.3036\n",
            "Epoch [6/50], Step [140/161], Loss: 1.1874\n",
            "Epoch [6/50], Step [150/161], Loss: 0.7233\n",
            "Epoch [6/50], Step [160/161], Loss: 1.0501\n",
            "Train accuracy is: 78.97196261682242 %\n",
            "validation accuracy is: 86.11111111111111 %\n",
            "Epoch [7/50], Step [10/161], Loss: 0.9527\n",
            "Epoch [7/50], Step [20/161], Loss: 0.8184\n",
            "Epoch [7/50], Step [30/161], Loss: 1.1607\n",
            "Epoch [7/50], Step [40/161], Loss: 1.3265\n",
            "Epoch [7/50], Step [50/161], Loss: 1.0879\n",
            "Epoch [7/50], Step [60/161], Loss: 0.8813\n",
            "Epoch [7/50], Step [70/161], Loss: 0.9561\n",
            "Epoch [7/50], Step [80/161], Loss: 1.0936\n",
            "Epoch [7/50], Step [90/161], Loss: 1.2003\n",
            "Epoch [7/50], Step [100/161], Loss: 0.7345\n",
            "Epoch [7/50], Step [110/161], Loss: 1.0390\n",
            "Epoch [7/50], Step [120/161], Loss: 0.8298\n",
            "Epoch [7/50], Step [130/161], Loss: 1.2348\n",
            "Epoch [7/50], Step [140/161], Loss: 0.7702\n",
            "Epoch [7/50], Step [150/161], Loss: 0.7994\n",
            "Epoch [7/50], Step [160/161], Loss: 0.7671\n",
            "Train accuracy is: 83.02180685358256 %\n",
            "validation accuracy is: 93.75 %\n",
            "Epoch [8/50], Step [10/161], Loss: 0.5539\n",
            "Epoch [8/50], Step [20/161], Loss: 1.1042\n",
            "Epoch [8/50], Step [30/161], Loss: 0.4248\n",
            "Epoch [8/50], Step [40/161], Loss: 0.9910\n",
            "Epoch [8/50], Step [50/161], Loss: 1.0467\n",
            "Epoch [8/50], Step [60/161], Loss: 0.9585\n",
            "Epoch [8/50], Step [70/161], Loss: 1.0350\n",
            "Epoch [8/50], Step [80/161], Loss: 1.3963\n",
            "Epoch [8/50], Step [90/161], Loss: 1.1065\n",
            "Epoch [8/50], Step [100/161], Loss: 0.9537\n",
            "Epoch [8/50], Step [110/161], Loss: 0.9788\n",
            "Epoch [8/50], Step [120/161], Loss: 1.0584\n",
            "Epoch [8/50], Step [130/161], Loss: 0.6248\n",
            "Epoch [8/50], Step [140/161], Loss: 0.7026\n",
            "Epoch [8/50], Step [150/161], Loss: 1.2614\n",
            "Epoch [8/50], Step [160/161], Loss: 0.7711\n",
            "Train accuracy is: 84.11214953271028 %\n",
            "validation accuracy is: 95.13888888888889 %\n",
            "Epoch [9/50], Step [10/161], Loss: 0.5553\n",
            "Epoch [9/50], Step [20/161], Loss: 0.8437\n",
            "Epoch [9/50], Step [30/161], Loss: 1.0114\n",
            "Epoch [9/50], Step [40/161], Loss: 0.6024\n",
            "Epoch [9/50], Step [50/161], Loss: 0.6681\n",
            "Epoch [9/50], Step [60/161], Loss: 1.2769\n",
            "Epoch [9/50], Step [70/161], Loss: 1.0183\n",
            "Epoch [9/50], Step [80/161], Loss: 0.6551\n",
            "Epoch [9/50], Step [90/161], Loss: 0.5844\n",
            "Epoch [9/50], Step [100/161], Loss: 0.8653\n",
            "Epoch [9/50], Step [110/161], Loss: 0.7801\n",
            "Epoch [9/50], Step [120/161], Loss: 0.8940\n",
            "Epoch [9/50], Step [130/161], Loss: 1.1217\n",
            "Epoch [9/50], Step [140/161], Loss: 1.0731\n",
            "Epoch [9/50], Step [150/161], Loss: 0.7510\n",
            "Epoch [9/50], Step [160/161], Loss: 0.3866\n",
            "Train accuracy is: 86.52647975077882 %\n",
            "validation accuracy is: 92.36111111111111 %\n",
            "Epoch [10/50], Step [10/161], Loss: 0.6065\n",
            "Epoch [10/50], Step [20/161], Loss: 0.5036\n",
            "Epoch [10/50], Step [30/161], Loss: 0.8228\n",
            "Epoch [10/50], Step [40/161], Loss: 1.2792\n",
            "Epoch [10/50], Step [50/161], Loss: 0.5814\n",
            "Epoch [10/50], Step [60/161], Loss: 1.1222\n",
            "Epoch [10/50], Step [70/161], Loss: 0.8887\n",
            "Epoch [10/50], Step [80/161], Loss: 1.0646\n",
            "Epoch [10/50], Step [90/161], Loss: 0.8759\n",
            "Epoch [10/50], Step [100/161], Loss: 0.8022\n",
            "Epoch [10/50], Step [110/161], Loss: 1.0429\n",
            "Epoch [10/50], Step [120/161], Loss: 0.7269\n",
            "Epoch [10/50], Step [130/161], Loss: 1.0757\n",
            "Epoch [10/50], Step [140/161], Loss: 0.1911\n",
            "Epoch [10/50], Step [150/161], Loss: 0.7790\n",
            "Epoch [10/50], Step [160/161], Loss: 0.8497\n",
            "Train accuracy is: 84.34579439252336 %\n",
            "validation accuracy is: 95.13888888888889 %\n",
            "Epoch [11/50], Step [10/161], Loss: 0.7150\n",
            "Epoch [11/50], Step [20/161], Loss: 0.4483\n",
            "Epoch [11/50], Step [30/161], Loss: 0.9556\n",
            "Epoch [11/50], Step [40/161], Loss: 0.5152\n",
            "Epoch [11/50], Step [50/161], Loss: 0.8847\n",
            "Epoch [11/50], Step [60/161], Loss: 1.2098\n",
            "Epoch [11/50], Step [70/161], Loss: 0.7492\n",
            "Epoch [11/50], Step [80/161], Loss: 1.3704\n",
            "Epoch [11/50], Step [90/161], Loss: 1.0239\n",
            "Epoch [11/50], Step [100/161], Loss: 0.5887\n",
            "Epoch [11/50], Step [110/161], Loss: 1.1384\n",
            "Epoch [11/50], Step [120/161], Loss: 0.8573\n",
            "Epoch [11/50], Step [130/161], Loss: 0.8513\n",
            "Epoch [11/50], Step [140/161], Loss: 1.1657\n",
            "Epoch [11/50], Step [150/161], Loss: 0.9856\n",
            "Epoch [11/50], Step [160/161], Loss: 0.8336\n",
            "Train accuracy is: 83.95638629283489 %\n",
            "validation accuracy is: 96.52777777777777 %\n",
            "Epoch [12/50], Step [10/161], Loss: 1.0442\n",
            "Epoch [12/50], Step [20/161], Loss: 0.5500\n",
            "Epoch [12/50], Step [30/161], Loss: 0.8881\n",
            "Epoch [12/50], Step [40/161], Loss: 0.8530\n",
            "Epoch [12/50], Step [50/161], Loss: 1.1576\n",
            "Epoch [12/50], Step [60/161], Loss: 0.5706\n",
            "Epoch [12/50], Step [70/161], Loss: 0.8202\n",
            "Epoch [12/50], Step [80/161], Loss: 0.8384\n",
            "Epoch [12/50], Step [90/161], Loss: 0.6552\n",
            "Epoch [12/50], Step [100/161], Loss: 1.0267\n",
            "Epoch [12/50], Step [110/161], Loss: 0.7691\n",
            "Epoch [12/50], Step [120/161], Loss: 0.7541\n",
            "Epoch [12/50], Step [130/161], Loss: 0.6381\n",
            "Epoch [12/50], Step [140/161], Loss: 0.6547\n",
            "Epoch [12/50], Step [150/161], Loss: 0.9136\n",
            "Epoch [12/50], Step [160/161], Loss: 0.6070\n",
            "Train accuracy is: 86.6822429906542 %\n",
            "validation accuracy is: 95.13888888888889 %\n",
            "Epoch [13/50], Step [10/161], Loss: 1.1291\n",
            "Epoch [13/50], Step [20/161], Loss: 0.5409\n",
            "Epoch [13/50], Step [30/161], Loss: 0.6096\n",
            "Epoch [13/50], Step [40/161], Loss: 1.5321\n",
            "Epoch [13/50], Step [50/161], Loss: 1.2812\n",
            "Epoch [13/50], Step [60/161], Loss: 0.7566\n",
            "Epoch [13/50], Step [70/161], Loss: 0.6977\n",
            "Epoch [13/50], Step [80/161], Loss: 1.2624\n",
            "Epoch [13/50], Step [90/161], Loss: 0.3602\n",
            "Epoch [13/50], Step [100/161], Loss: 0.7987\n",
            "Epoch [13/50], Step [110/161], Loss: 0.6515\n",
            "Epoch [13/50], Step [120/161], Loss: 0.8416\n",
            "Epoch [13/50], Step [130/161], Loss: 0.2805\n",
            "Epoch [13/50], Step [140/161], Loss: 0.9397\n",
            "Epoch [13/50], Step [150/161], Loss: 1.9650\n",
            "Epoch [13/50], Step [160/161], Loss: 0.6919\n",
            "Train accuracy is: 86.52647975077882 %\n",
            "validation accuracy is: 93.75 %\n",
            "Epoch [14/50], Step [10/161], Loss: 0.8302\n",
            "Epoch [14/50], Step [20/161], Loss: 1.0229\n",
            "Epoch [14/50], Step [30/161], Loss: 0.9234\n",
            "Epoch [14/50], Step [40/161], Loss: 0.6957\n",
            "Epoch [14/50], Step [50/161], Loss: 0.6374\n",
            "Epoch [14/50], Step [60/161], Loss: 0.9835\n",
            "Epoch [14/50], Step [70/161], Loss: 1.2506\n",
            "Epoch [14/50], Step [80/161], Loss: 0.8291\n",
            "Epoch [14/50], Step [90/161], Loss: 0.8261\n",
            "Epoch [14/50], Step [100/161], Loss: 1.1258\n",
            "Epoch [14/50], Step [110/161], Loss: 0.7638\n",
            "Epoch [14/50], Step [120/161], Loss: 0.7671\n",
            "Epoch [14/50], Step [130/161], Loss: 0.9928\n",
            "Epoch [14/50], Step [140/161], Loss: 1.1638\n",
            "Epoch [14/50], Step [150/161], Loss: 0.9405\n",
            "Epoch [14/50], Step [160/161], Loss: 0.8641\n",
            "Train accuracy is: 87.30529595015577 %\n",
            "validation accuracy is: 96.52777777777777 %\n",
            "Epoch [15/50], Step [10/161], Loss: 0.4657\n",
            "Epoch [15/50], Step [20/161], Loss: 0.1673\n",
            "Epoch [15/50], Step [30/161], Loss: 1.1298\n",
            "Epoch [15/50], Step [40/161], Loss: 0.7112\n",
            "Epoch [15/50], Step [50/161], Loss: 0.7466\n",
            "Epoch [15/50], Step [60/161], Loss: 1.2078\n",
            "Epoch [15/50], Step [70/161], Loss: 0.8872\n",
            "Epoch [15/50], Step [80/161], Loss: 0.5331\n",
            "Epoch [15/50], Step [90/161], Loss: 0.7946\n",
            "Epoch [15/50], Step [100/161], Loss: 0.9956\n",
            "Epoch [15/50], Step [110/161], Loss: 1.3487\n",
            "Epoch [15/50], Step [120/161], Loss: 0.9264\n",
            "Epoch [15/50], Step [130/161], Loss: 2.0452\n",
            "Epoch [15/50], Step [140/161], Loss: 0.8254\n",
            "Epoch [15/50], Step [150/161], Loss: 0.6146\n",
            "Epoch [15/50], Step [160/161], Loss: 0.7866\n",
            "Train accuracy is: 90.10903426791278 %\n",
            "validation accuracy is: 99.30555555555556 %\n",
            "Epoch [16/50], Step [10/161], Loss: 1.0223\n",
            "Epoch [16/50], Step [20/161], Loss: 1.1133\n",
            "Epoch [16/50], Step [30/161], Loss: 0.7506\n",
            "Epoch [16/50], Step [40/161], Loss: 0.5693\n",
            "Epoch [16/50], Step [50/161], Loss: 0.7932\n",
            "Epoch [16/50], Step [60/161], Loss: 0.4976\n",
            "Epoch [16/50], Step [70/161], Loss: 1.1744\n",
            "Epoch [16/50], Step [80/161], Loss: 0.8306\n",
            "Epoch [16/50], Step [90/161], Loss: 0.7302\n",
            "Epoch [16/50], Step [100/161], Loss: 0.7231\n",
            "Epoch [16/50], Step [110/161], Loss: 0.7158\n",
            "Epoch [16/50], Step [120/161], Loss: 1.0280\n",
            "Epoch [16/50], Step [130/161], Loss: 0.2317\n",
            "Epoch [16/50], Step [140/161], Loss: 0.7853\n",
            "Epoch [16/50], Step [150/161], Loss: 0.5034\n",
            "Epoch [16/50], Step [160/161], Loss: 1.1324\n",
            "Train accuracy is: 89.17445482866043 %\n",
            "validation accuracy is: 90.27777777777777 %\n",
            "Epoch [17/50], Step [10/161], Loss: 0.1696\n",
            "Epoch [17/50], Step [20/161], Loss: 0.9982\n",
            "Epoch [17/50], Step [30/161], Loss: 0.8154\n",
            "Epoch [17/50], Step [40/161], Loss: 0.7331\n",
            "Epoch [17/50], Step [50/161], Loss: 0.7356\n",
            "Epoch [17/50], Step [60/161], Loss: 0.8519\n",
            "Epoch [17/50], Step [70/161], Loss: 0.6847\n",
            "Epoch [17/50], Step [80/161], Loss: 0.4894\n",
            "Epoch [17/50], Step [90/161], Loss: 0.4603\n",
            "Epoch [17/50], Step [100/161], Loss: 0.7676\n",
            "Epoch [17/50], Step [110/161], Loss: 1.0147\n",
            "Epoch [17/50], Step [120/161], Loss: 0.8205\n",
            "Epoch [17/50], Step [130/161], Loss: 0.8883\n",
            "Epoch [17/50], Step [140/161], Loss: 0.2620\n",
            "Epoch [17/50], Step [150/161], Loss: 1.3074\n",
            "Epoch [17/50], Step [160/161], Loss: 0.7136\n",
            "Train accuracy is: 89.95327102803738 %\n",
            "validation accuracy is: 98.61111111111111 %\n",
            "Epoch [18/50], Step [10/161], Loss: 0.9312\n",
            "Epoch [18/50], Step [20/161], Loss: 0.6328\n",
            "Epoch [18/50], Step [30/161], Loss: 0.8073\n",
            "Epoch [18/50], Step [40/161], Loss: 2.0738\n",
            "Epoch [18/50], Step [50/161], Loss: 0.7152\n",
            "Epoch [18/50], Step [60/161], Loss: 0.6765\n",
            "Epoch [18/50], Step [70/161], Loss: 0.4981\n",
            "Epoch [18/50], Step [80/161], Loss: 0.7719\n",
            "Epoch [18/50], Step [90/161], Loss: 0.6341\n",
            "Epoch [18/50], Step [100/161], Loss: 0.7245\n",
            "Epoch [18/50], Step [110/161], Loss: 0.8486\n",
            "Epoch [18/50], Step [120/161], Loss: 0.7919\n",
            "Epoch [18/50], Step [130/161], Loss: 1.2965\n",
            "Epoch [18/50], Step [140/161], Loss: 1.0510\n",
            "Epoch [18/50], Step [150/161], Loss: 0.2318\n",
            "Epoch [18/50], Step [160/161], Loss: 0.8287\n",
            "Train accuracy is: 88.47352024922118 %\n",
            "validation accuracy is: 98.61111111111111 %\n",
            "Epoch [19/50], Step [10/161], Loss: 0.7482\n",
            "Epoch [19/50], Step [20/161], Loss: 0.8522\n",
            "Epoch [19/50], Step [30/161], Loss: 1.3890\n",
            "Epoch [19/50], Step [40/161], Loss: 0.6661\n",
            "Epoch [19/50], Step [50/161], Loss: 1.0016\n",
            "Epoch [19/50], Step [60/161], Loss: 0.8759\n",
            "Epoch [19/50], Step [70/161], Loss: 0.7915\n",
            "Epoch [19/50], Step [80/161], Loss: 0.6561\n",
            "Epoch [19/50], Step [90/161], Loss: 0.6054\n",
            "Epoch [19/50], Step [100/161], Loss: 0.4602\n",
            "Epoch [19/50], Step [110/161], Loss: 0.6040\n",
            "Epoch [19/50], Step [120/161], Loss: 0.6940\n",
            "Epoch [19/50], Step [130/161], Loss: 0.6133\n",
            "Epoch [19/50], Step [140/161], Loss: 0.8315\n",
            "Epoch [19/50], Step [150/161], Loss: 0.4695\n",
            "Epoch [19/50], Step [160/161], Loss: 0.7466\n",
            "Train accuracy is: 91.1214953271028 %\n",
            "validation accuracy is: 98.61111111111111 %\n",
            "Epoch [20/50], Step [10/161], Loss: 0.8248\n",
            "Epoch [20/50], Step [20/161], Loss: 0.8561\n",
            "Epoch [20/50], Step [30/161], Loss: 0.9755\n",
            "Epoch [20/50], Step [40/161], Loss: 0.9518\n",
            "Epoch [20/50], Step [50/161], Loss: 0.5526\n",
            "Epoch [20/50], Step [60/161], Loss: 0.9804\n",
            "Epoch [20/50], Step [70/161], Loss: 0.7140\n",
            "Epoch [20/50], Step [80/161], Loss: 0.1327\n",
            "Epoch [20/50], Step [90/161], Loss: 0.6011\n",
            "Epoch [20/50], Step [100/161], Loss: 0.5146\n",
            "Epoch [20/50], Step [110/161], Loss: 0.8951\n",
            "Epoch [20/50], Step [120/161], Loss: 0.6642\n",
            "Epoch [20/50], Step [130/161], Loss: 0.4726\n",
            "Epoch [20/50], Step [140/161], Loss: 0.6726\n",
            "Epoch [20/50], Step [150/161], Loss: 0.4464\n",
            "Epoch [20/50], Step [160/161], Loss: 1.3219\n",
            "Train accuracy is: 89.64174454828661 %\n",
            "validation accuracy is: 95.13888888888889 %\n",
            "Epoch [21/50], Step [10/161], Loss: 0.5349\n",
            "Epoch [21/50], Step [20/161], Loss: 0.5097\n",
            "Epoch [21/50], Step [30/161], Loss: 0.4479\n",
            "Epoch [21/50], Step [40/161], Loss: 0.7122\n",
            "Epoch [21/50], Step [50/161], Loss: 0.4952\n",
            "Epoch [21/50], Step [60/161], Loss: 1.0997\n",
            "Epoch [21/50], Step [70/161], Loss: 0.8531\n",
            "Epoch [21/50], Step [80/161], Loss: 0.4502\n",
            "Epoch [21/50], Step [90/161], Loss: 0.5267\n",
            "Epoch [21/50], Step [100/161], Loss: 0.5302\n",
            "Epoch [21/50], Step [110/161], Loss: 0.6850\n",
            "Epoch [21/50], Step [120/161], Loss: 0.6708\n",
            "Epoch [21/50], Step [130/161], Loss: 0.5219\n",
            "Epoch [21/50], Step [140/161], Loss: 0.7327\n",
            "Epoch [21/50], Step [150/161], Loss: 0.5891\n",
            "Epoch [21/50], Step [160/161], Loss: 2.0784\n",
            "Train accuracy is: 91.1214953271028 %\n",
            "validation accuracy is: 97.91666666666667 %\n",
            "Epoch [22/50], Step [10/161], Loss: 1.3322\n",
            "Epoch [22/50], Step [20/161], Loss: 0.5379\n",
            "Epoch [22/50], Step [30/161], Loss: 0.8693\n",
            "Epoch [22/50], Step [40/161], Loss: 0.3273\n",
            "Epoch [22/50], Step [50/161], Loss: 0.7292\n",
            "Epoch [22/50], Step [60/161], Loss: 1.1437\n",
            "Epoch [22/50], Step [70/161], Loss: 0.4991\n",
            "Epoch [22/50], Step [80/161], Loss: 1.2332\n",
            "Epoch [22/50], Step [90/161], Loss: 0.5640\n",
            "Epoch [22/50], Step [100/161], Loss: 1.3715\n",
            "Epoch [22/50], Step [110/161], Loss: 1.0902\n",
            "Epoch [22/50], Step [120/161], Loss: 0.7155\n",
            "Epoch [22/50], Step [130/161], Loss: 0.9582\n",
            "Epoch [22/50], Step [140/161], Loss: 0.5971\n",
            "Epoch [22/50], Step [150/161], Loss: 0.6786\n",
            "Epoch [22/50], Step [160/161], Loss: 0.6765\n",
            "Train accuracy is: 90.57632398753894 %\n",
            "validation accuracy is: 96.52777777777777 %\n",
            "Epoch [23/50], Step [10/161], Loss: 0.7628\n",
            "Epoch [23/50], Step [20/161], Loss: 0.5321\n",
            "Epoch [23/50], Step [30/161], Loss: 1.3440\n",
            "Epoch [23/50], Step [40/161], Loss: 0.7207\n",
            "Epoch [23/50], Step [50/161], Loss: 1.1001\n",
            "Epoch [23/50], Step [60/161], Loss: 0.4485\n",
            "Epoch [23/50], Step [70/161], Loss: 0.5712\n",
            "Epoch [23/50], Step [80/161], Loss: 0.7135\n",
            "Epoch [23/50], Step [90/161], Loss: 0.9798\n",
            "Epoch [23/50], Step [100/161], Loss: 0.8194\n",
            "Epoch [23/50], Step [110/161], Loss: 0.8017\n",
            "Epoch [23/50], Step [120/161], Loss: 0.6550\n",
            "Epoch [23/50], Step [130/161], Loss: 0.6854\n",
            "Epoch [23/50], Step [140/161], Loss: 0.8088\n",
            "Epoch [23/50], Step [150/161], Loss: 0.5756\n",
            "Epoch [23/50], Step [160/161], Loss: 0.9229\n",
            "Train accuracy is: 90.57632398753894 %\n",
            "validation accuracy is: 98.61111111111111 %\n",
            "Epoch [24/50], Step [10/161], Loss: 0.6574\n",
            "Epoch [24/50], Step [20/161], Loss: 0.4709\n",
            "Epoch [24/50], Step [30/161], Loss: 0.4857\n",
            "Epoch [24/50], Step [40/161], Loss: 0.7629\n",
            "Epoch [24/50], Step [50/161], Loss: 0.6755\n",
            "Epoch [24/50], Step [60/161], Loss: 0.6102\n",
            "Epoch [24/50], Step [70/161], Loss: 0.8216\n",
            "Epoch [24/50], Step [80/161], Loss: 1.0577\n",
            "Epoch [24/50], Step [90/161], Loss: 1.0729\n",
            "Epoch [24/50], Step [100/161], Loss: 0.4713\n",
            "Epoch [24/50], Step [110/161], Loss: 0.9294\n",
            "Epoch [24/50], Step [120/161], Loss: 0.5745\n",
            "Epoch [24/50], Step [130/161], Loss: 0.7079\n",
            "Epoch [24/50], Step [140/161], Loss: 1.0130\n",
            "Epoch [24/50], Step [150/161], Loss: 0.4402\n",
            "Epoch [24/50], Step [160/161], Loss: 0.6614\n",
            "Train accuracy is: 90.49844236760124 %\n",
            "validation accuracy is: 95.13888888888889 %\n",
            "Epoch [25/50], Step [10/161], Loss: 0.7781\n",
            "Epoch [25/50], Step [20/161], Loss: 0.7187\n",
            "Epoch [25/50], Step [30/161], Loss: 0.7353\n",
            "Epoch [25/50], Step [40/161], Loss: 0.8916\n",
            "Epoch [25/50], Step [50/161], Loss: 0.7204\n",
            "Epoch [25/50], Step [60/161], Loss: 0.6135\n",
            "Epoch [25/50], Step [70/161], Loss: 1.0638\n",
            "Epoch [25/50], Step [80/161], Loss: 0.6687\n",
            "Epoch [25/50], Step [90/161], Loss: 0.6075\n",
            "Epoch [25/50], Step [100/161], Loss: 1.3838\n",
            "Epoch [25/50], Step [110/161], Loss: 0.9537\n",
            "Epoch [25/50], Step [120/161], Loss: 0.7196\n",
            "Epoch [25/50], Step [130/161], Loss: 0.9533\n",
            "Epoch [25/50], Step [140/161], Loss: 0.7317\n",
            "Epoch [25/50], Step [150/161], Loss: 0.7504\n",
            "Epoch [25/50], Step [160/161], Loss: 0.9983\n",
            "Train accuracy is: 92.28971962616822 %\n",
            "validation accuracy is: 97.22222222222223 %\n",
            "Epoch [26/50], Step [10/161], Loss: 0.5619\n",
            "Epoch [26/50], Step [20/161], Loss: 0.8787\n",
            "Epoch [26/50], Step [30/161], Loss: 0.7949\n",
            "Epoch [26/50], Step [40/161], Loss: 0.6473\n",
            "Epoch [26/50], Step [50/161], Loss: 1.1924\n",
            "Epoch [26/50], Step [60/161], Loss: 1.9336\n",
            "Epoch [26/50], Step [70/161], Loss: 0.6975\n",
            "Epoch [26/50], Step [80/161], Loss: 0.6691\n",
            "Epoch [26/50], Step [90/161], Loss: 0.9032\n",
            "Epoch [26/50], Step [100/161], Loss: 0.1826\n",
            "Epoch [26/50], Step [110/161], Loss: 0.6132\n",
            "Epoch [26/50], Step [120/161], Loss: 1.3951\n",
            "Epoch [26/50], Step [130/161], Loss: 0.7077\n",
            "Epoch [26/50], Step [140/161], Loss: 0.2472\n",
            "Epoch [26/50], Step [150/161], Loss: 0.5984\n",
            "Epoch [26/50], Step [160/161], Loss: 0.4536\n",
            "Train accuracy is: 91.1214953271028 %\n",
            "validation accuracy is: 98.61111111111111 %\n",
            "Epoch [27/50], Step [10/161], Loss: 0.6893\n",
            "Epoch [27/50], Step [20/161], Loss: 0.4312\n",
            "Epoch [27/50], Step [30/161], Loss: 0.6691\n",
            "Epoch [27/50], Step [40/161], Loss: 0.0905\n",
            "Epoch [27/50], Step [50/161], Loss: 1.1302\n",
            "Epoch [27/50], Step [60/161], Loss: 1.0255\n",
            "Epoch [27/50], Step [70/161], Loss: 0.6507\n",
            "Epoch [27/50], Step [80/161], Loss: 0.7245\n",
            "Epoch [27/50], Step [90/161], Loss: 0.3736\n",
            "Epoch [27/50], Step [100/161], Loss: 0.0933\n",
            "Epoch [27/50], Step [110/161], Loss: 0.8914\n",
            "Epoch [27/50], Step [120/161], Loss: 0.6869\n",
            "Epoch [27/50], Step [130/161], Loss: 0.7763\n",
            "Epoch [27/50], Step [140/161], Loss: 0.4817\n",
            "Epoch [27/50], Step [150/161], Loss: 0.1108\n",
            "Epoch [27/50], Step [160/161], Loss: 0.7467\n",
            "Train accuracy is: 92.36760124610592 %\n",
            "validation accuracy is: 97.91666666666667 %\n",
            "Epoch [28/50], Step [10/161], Loss: 0.5792\n",
            "Epoch [28/50], Step [20/161], Loss: 0.9255\n",
            "Epoch [28/50], Step [30/161], Loss: 0.8343\n",
            "Epoch [28/50], Step [40/161], Loss: 0.5089\n",
            "Epoch [28/50], Step [50/161], Loss: 1.4368\n",
            "Epoch [28/50], Step [60/161], Loss: 0.6361\n",
            "Epoch [28/50], Step [70/161], Loss: 0.8004\n",
            "Epoch [28/50], Step [80/161], Loss: 1.1671\n",
            "Epoch [28/50], Step [90/161], Loss: 0.6443\n",
            "Epoch [28/50], Step [100/161], Loss: 0.5357\n",
            "Epoch [28/50], Step [110/161], Loss: 0.0837\n",
            "Epoch [28/50], Step [120/161], Loss: 0.6775\n",
            "Epoch [28/50], Step [130/161], Loss: 0.7869\n",
            "Epoch [28/50], Step [140/161], Loss: 0.6657\n",
            "Epoch [28/50], Step [150/161], Loss: 0.4626\n",
            "Epoch [28/50], Step [160/161], Loss: 0.7752\n",
            "Train accuracy is: 90.73208722741433 %\n",
            "validation accuracy is: 97.91666666666667 %\n",
            "Epoch [29/50], Step [10/161], Loss: 0.6702\n",
            "Epoch [29/50], Step [20/161], Loss: 0.4781\n",
            "Epoch [29/50], Step [30/161], Loss: 0.7700\n",
            "Epoch [29/50], Step [40/161], Loss: 0.4230\n",
            "Epoch [29/50], Step [50/161], Loss: 0.9344\n",
            "Epoch [29/50], Step [60/161], Loss: 0.9362\n",
            "Epoch [29/50], Step [70/161], Loss: 0.9465\n",
            "Epoch [29/50], Step [80/161], Loss: 0.7335\n",
            "Epoch [29/50], Step [90/161], Loss: 0.4534\n",
            "Epoch [29/50], Step [100/161], Loss: 0.4423\n",
            "Epoch [29/50], Step [110/161], Loss: 0.7873\n",
            "Epoch [29/50], Step [120/161], Loss: 0.7020\n",
            "Epoch [29/50], Step [130/161], Loss: 0.6221\n",
            "Epoch [29/50], Step [140/161], Loss: 0.6166\n",
            "Epoch [29/50], Step [150/161], Loss: 0.7319\n",
            "Epoch [29/50], Step [160/161], Loss: 1.3576\n",
            "Train accuracy is: 90.57632398753894 %\n",
            "validation accuracy is: 100.0 %\n",
            "Epoch [30/50], Step [10/161], Loss: 0.4717\n",
            "Epoch [30/50], Step [20/161], Loss: 0.4879\n",
            "Epoch [30/50], Step [30/161], Loss: 0.4593\n",
            "Epoch [30/50], Step [40/161], Loss: 0.7477\n",
            "Epoch [30/50], Step [50/161], Loss: 0.7520\n",
            "Epoch [30/50], Step [60/161], Loss: 0.5818\n",
            "Epoch [30/50], Step [70/161], Loss: 0.6914\n",
            "Epoch [30/50], Step [80/161], Loss: 0.6479\n",
            "Epoch [30/50], Step [90/161], Loss: 1.2961\n",
            "Epoch [30/50], Step [100/161], Loss: 0.7895\n",
            "Epoch [30/50], Step [110/161], Loss: 1.1005\n",
            "Epoch [30/50], Step [120/161], Loss: 1.0277\n",
            "Epoch [30/50], Step [130/161], Loss: 0.6466\n",
            "Epoch [30/50], Step [140/161], Loss: 0.6318\n",
            "Epoch [30/50], Step [150/161], Loss: 0.7212\n",
            "Epoch [30/50], Step [160/161], Loss: 0.6544\n",
            "Train accuracy is: 92.44548286604362 %\n",
            "validation accuracy is: 96.52777777777777 %\n",
            "Epoch [31/50], Step [10/161], Loss: 0.6223\n",
            "Epoch [31/50], Step [20/161], Loss: 0.7896\n",
            "Epoch [31/50], Step [30/161], Loss: 0.7648\n",
            "Epoch [31/50], Step [40/161], Loss: 0.4149\n",
            "Epoch [31/50], Step [50/161], Loss: 1.4648\n",
            "Epoch [31/50], Step [60/161], Loss: 0.7836\n",
            "Epoch [31/50], Step [70/161], Loss: 0.4477\n",
            "Epoch [31/50], Step [80/161], Loss: 0.7577\n",
            "Epoch [31/50], Step [90/161], Loss: 1.0797\n",
            "Epoch [31/50], Step [100/161], Loss: 0.9293\n",
            "Epoch [31/50], Step [110/161], Loss: 0.5743\n",
            "Epoch [31/50], Step [120/161], Loss: 0.6257\n",
            "Epoch [31/50], Step [130/161], Loss: 0.9844\n",
            "Epoch [31/50], Step [140/161], Loss: 0.5966\n",
            "Epoch [31/50], Step [150/161], Loss: 0.9098\n",
            "Epoch [31/50], Step [160/161], Loss: 0.7179\n",
            "Train accuracy is: 91.58878504672897 %\n",
            "validation accuracy is: 99.30555555555556 %\n",
            "Epoch [32/50], Step [10/161], Loss: 0.4339\n",
            "Epoch [32/50], Step [20/161], Loss: 0.4598\n",
            "Epoch [32/50], Step [30/161], Loss: 0.6509\n",
            "Epoch [32/50], Step [40/161], Loss: 0.4760\n",
            "Epoch [32/50], Step [50/161], Loss: 0.6245\n",
            "Epoch [32/50], Step [60/161], Loss: 0.8519\n",
            "Epoch [32/50], Step [70/161], Loss: 0.9830\n",
            "Epoch [32/50], Step [80/161], Loss: 0.5972\n",
            "Epoch [32/50], Step [90/161], Loss: 0.6081\n",
            "Epoch [32/50], Step [100/161], Loss: 0.6651\n",
            "Epoch [32/50], Step [110/161], Loss: 0.8415\n",
            "Epoch [32/50], Step [120/161], Loss: 0.5946\n",
            "Epoch [32/50], Step [130/161], Loss: 0.7896\n",
            "Epoch [32/50], Step [140/161], Loss: 0.8685\n",
            "Epoch [32/50], Step [150/161], Loss: 0.6313\n",
            "Epoch [32/50], Step [160/161], Loss: 0.4820\n",
            "Train accuracy is: 92.601246105919 %\n",
            "validation accuracy is: 100.0 %\n",
            "Epoch [33/50], Step [10/161], Loss: 0.5473\n",
            "Epoch [33/50], Step [20/161], Loss: 0.6017\n",
            "Epoch [33/50], Step [30/161], Loss: 0.6218\n",
            "Epoch [33/50], Step [40/161], Loss: 0.5907\n",
            "Epoch [33/50], Step [50/161], Loss: 0.5874\n",
            "Epoch [33/50], Step [60/161], Loss: 0.7431\n",
            "Epoch [33/50], Step [70/161], Loss: 0.5813\n",
            "Epoch [33/50], Step [80/161], Loss: 0.5045\n",
            "Epoch [33/50], Step [90/161], Loss: 1.4434\n",
            "Epoch [33/50], Step [100/161], Loss: 0.7471\n",
            "Epoch [33/50], Step [110/161], Loss: 0.6375\n",
            "Epoch [33/50], Step [120/161], Loss: 0.7464\n",
            "Epoch [33/50], Step [130/161], Loss: 0.1156\n",
            "Epoch [33/50], Step [140/161], Loss: 0.5858\n",
            "Epoch [33/50], Step [150/161], Loss: 0.8655\n",
            "Epoch [33/50], Step [160/161], Loss: 0.5637\n",
            "Train accuracy is: 92.75700934579439 %\n",
            "validation accuracy is: 100.0 %\n",
            "Epoch [34/50], Step [10/161], Loss: 0.6816\n",
            "Epoch [34/50], Step [20/161], Loss: 0.5692\n",
            "Epoch [34/50], Step [30/161], Loss: 0.6167\n",
            "Epoch [34/50], Step [40/161], Loss: 0.6376\n",
            "Epoch [34/50], Step [50/161], Loss: 0.5804\n",
            "Epoch [34/50], Step [60/161], Loss: 0.6000\n",
            "Epoch [34/50], Step [70/161], Loss: 0.5853\n",
            "Epoch [34/50], Step [80/161], Loss: 0.7558\n",
            "Epoch [34/50], Step [90/161], Loss: 0.5125\n",
            "Epoch [34/50], Step [100/161], Loss: 0.5429\n",
            "Epoch [34/50], Step [110/161], Loss: 0.6401\n",
            "Epoch [34/50], Step [120/161], Loss: 0.4302\n",
            "Epoch [34/50], Step [130/161], Loss: 0.6379\n",
            "Epoch [34/50], Step [140/161], Loss: 0.4235\n",
            "Epoch [34/50], Step [150/161], Loss: 0.7958\n",
            "Epoch [34/50], Step [160/161], Loss: 0.4480\n",
            "Train accuracy is: 92.75700934579439 %\n",
            "validation accuracy is: 100.0 %\n",
            "Epoch [35/50], Step [10/161], Loss: 0.4635\n",
            "Epoch [35/50], Step [20/161], Loss: 0.6586\n",
            "Epoch [35/50], Step [30/161], Loss: 1.2232\n",
            "Epoch [35/50], Step [40/161], Loss: 0.6115\n",
            "Epoch [35/50], Step [50/161], Loss: 1.0179\n",
            "Epoch [35/50], Step [60/161], Loss: 0.6836\n",
            "Epoch [35/50], Step [70/161], Loss: 1.0302\n",
            "Epoch [35/50], Step [80/161], Loss: 0.7275\n",
            "Epoch [35/50], Step [90/161], Loss: 0.4890\n",
            "Epoch [35/50], Step [100/161], Loss: 0.5617\n",
            "Epoch [35/50], Step [110/161], Loss: 0.4432\n",
            "Epoch [35/50], Step [120/161], Loss: 0.6008\n",
            "Epoch [35/50], Step [130/161], Loss: 0.9385\n",
            "Epoch [35/50], Step [140/161], Loss: 0.4441\n",
            "Epoch [35/50], Step [150/161], Loss: 0.9922\n",
            "Epoch [35/50], Step [160/161], Loss: 0.4440\n",
            "Train accuracy is: 91.43302180685359 %\n",
            "validation accuracy is: 97.91666666666667 %\n",
            "Epoch [36/50], Step [10/161], Loss: 0.5142\n",
            "Epoch [36/50], Step [20/161], Loss: 0.6841\n",
            "Epoch [36/50], Step [30/161], Loss: 0.6236\n",
            "Epoch [36/50], Step [40/161], Loss: 0.8393\n",
            "Epoch [36/50], Step [50/161], Loss: 1.7346\n",
            "Epoch [36/50], Step [60/161], Loss: 0.6366\n",
            "Epoch [36/50], Step [70/161], Loss: 0.4799\n",
            "Epoch [36/50], Step [80/161], Loss: 0.6231\n",
            "Epoch [36/50], Step [90/161], Loss: 0.7181\n",
            "Epoch [36/50], Step [100/161], Loss: 0.6343\n",
            "Epoch [36/50], Step [110/161], Loss: 0.5949\n",
            "Epoch [36/50], Step [120/161], Loss: 0.5820\n",
            "Epoch [36/50], Step [130/161], Loss: 0.5856\n",
            "Epoch [36/50], Step [140/161], Loss: 0.4512\n",
            "Epoch [36/50], Step [150/161], Loss: 0.7147\n",
            "Epoch [36/50], Step [160/161], Loss: 0.4673\n",
            "Train accuracy is: 92.91277258566979 %\n",
            "validation accuracy is: 98.61111111111111 %\n",
            "Epoch [37/50], Step [10/161], Loss: 1.3494\n",
            "Epoch [37/50], Step [20/161], Loss: 0.5921\n",
            "Epoch [37/50], Step [30/161], Loss: 1.1421\n",
            "Epoch [37/50], Step [40/161], Loss: 0.8789\n",
            "Epoch [37/50], Step [50/161], Loss: 0.6009\n",
            "Epoch [37/50], Step [60/161], Loss: 0.4253\n",
            "Epoch [37/50], Step [70/161], Loss: 0.7946\n",
            "Epoch [37/50], Step [80/161], Loss: 0.9284\n",
            "Epoch [37/50], Step [90/161], Loss: 0.6359\n",
            "Epoch [37/50], Step [100/161], Loss: 0.4758\n",
            "Epoch [37/50], Step [110/161], Loss: 0.6191\n",
            "Epoch [37/50], Step [120/161], Loss: 0.6091\n",
            "Epoch [37/50], Step [130/161], Loss: 0.4221\n",
            "Epoch [37/50], Step [140/161], Loss: 0.5758\n",
            "Epoch [37/50], Step [150/161], Loss: 0.5886\n",
            "Epoch [37/50], Step [160/161], Loss: 0.5679\n",
            "Train accuracy is: 90.88785046728972 %\n",
            "validation accuracy is: 99.30555555555556 %\n",
            "Epoch [38/50], Step [10/161], Loss: 0.2695\n",
            "Epoch [38/50], Step [20/161], Loss: 0.3327\n",
            "Epoch [38/50], Step [30/161], Loss: 0.4358\n",
            "Epoch [38/50], Step [40/161], Loss: 0.5063\n",
            "Epoch [38/50], Step [50/161], Loss: 0.0878\n",
            "Epoch [38/50], Step [60/161], Loss: 0.7543\n",
            "Epoch [38/50], Step [70/161], Loss: 0.4719\n",
            "Epoch [38/50], Step [80/161], Loss: 0.9053\n",
            "Epoch [38/50], Step [90/161], Loss: 0.6265\n",
            "Epoch [38/50], Step [100/161], Loss: 1.4278\n",
            "Epoch [38/50], Step [110/161], Loss: 1.0505\n",
            "Epoch [38/50], Step [120/161], Loss: 0.4723\n",
            "Epoch [38/50], Step [130/161], Loss: 0.6364\n",
            "Epoch [38/50], Step [140/161], Loss: 0.5370\n",
            "Epoch [38/50], Step [150/161], Loss: 0.6469\n",
            "Epoch [38/50], Step [160/161], Loss: 0.5754\n",
            "Train accuracy is: 93.30218068535825 %\n",
            "validation accuracy is: 100.0 %\n",
            "Epoch [39/50], Step [10/161], Loss: 1.0577\n",
            "Epoch [39/50], Step [20/161], Loss: 0.6787\n",
            "Epoch [39/50], Step [30/161], Loss: 0.4317\n",
            "Epoch [39/50], Step [40/161], Loss: 0.5592\n",
            "Epoch [39/50], Step [50/161], Loss: 0.5673\n",
            "Epoch [39/50], Step [60/161], Loss: 1.1486\n",
            "Epoch [39/50], Step [70/161], Loss: 0.5807\n",
            "Epoch [39/50], Step [80/161], Loss: 0.6404\n",
            "Epoch [39/50], Step [90/161], Loss: 0.8432\n",
            "Epoch [39/50], Step [100/161], Loss: 0.6300\n",
            "Epoch [39/50], Step [110/161], Loss: 0.6407\n",
            "Epoch [39/50], Step [120/161], Loss: 0.6689\n",
            "Epoch [39/50], Step [130/161], Loss: 0.4812\n",
            "Epoch [39/50], Step [140/161], Loss: 1.4079\n",
            "Epoch [39/50], Step [150/161], Loss: 0.6914\n",
            "Epoch [39/50], Step [160/161], Loss: 0.4493\n",
            "Train accuracy is: 92.05607476635514 %\n",
            "validation accuracy is: 99.30555555555556 %\n",
            "Epoch [40/50], Step [10/161], Loss: 0.6697\n",
            "Epoch [40/50], Step [20/161], Loss: 0.4623\n",
            "Epoch [40/50], Step [30/161], Loss: 1.0287\n",
            "Epoch [40/50], Step [40/161], Loss: 0.4771\n",
            "Epoch [40/50], Step [50/161], Loss: 0.4573\n",
            "Epoch [40/50], Step [60/161], Loss: 1.1355\n",
            "Epoch [40/50], Step [70/161], Loss: 0.5648\n",
            "Epoch [40/50], Step [80/161], Loss: 0.8822\n",
            "Epoch [40/50], Step [90/161], Loss: 0.6252\n",
            "Epoch [40/50], Step [100/161], Loss: 0.5951\n",
            "Epoch [40/50], Step [110/161], Loss: 0.5383\n",
            "Epoch [40/50], Step [120/161], Loss: 1.7809\n",
            "Epoch [40/50], Step [130/161], Loss: 1.0024\n",
            "Epoch [40/50], Step [140/161], Loss: 0.5724\n",
            "Epoch [40/50], Step [150/161], Loss: 1.1146\n",
            "Epoch [40/50], Step [160/161], Loss: 1.2629\n",
            "Train accuracy is: 92.21183800623054 %\n",
            "validation accuracy is: 100.0 %\n",
            "Epoch [41/50], Step [10/161], Loss: 0.1500\n",
            "Epoch [41/50], Step [20/161], Loss: 0.7066\n",
            "Epoch [41/50], Step [30/161], Loss: 1.4154\n",
            "Epoch [41/50], Step [40/161], Loss: 0.8114\n",
            "Epoch [41/50], Step [50/161], Loss: 0.6083\n",
            "Epoch [41/50], Step [60/161], Loss: 1.2020\n",
            "Epoch [41/50], Step [70/161], Loss: 0.6650\n",
            "Epoch [41/50], Step [80/161], Loss: 0.2985\n",
            "Epoch [41/50], Step [90/161], Loss: 0.2245\n",
            "Epoch [41/50], Step [100/161], Loss: 0.4435\n",
            "Epoch [41/50], Step [110/161], Loss: 0.8197\n",
            "Epoch [41/50], Step [120/161], Loss: 0.5863\n",
            "Epoch [41/50], Step [130/161], Loss: 1.0926\n",
            "Epoch [41/50], Step [140/161], Loss: 0.4085\n",
            "Epoch [41/50], Step [150/161], Loss: 0.8995\n",
            "Epoch [41/50], Step [160/161], Loss: 0.4521\n",
            "Train accuracy is: 91.74454828660436 %\n",
            "validation accuracy is: 100.0 %\n",
            "Epoch [42/50], Step [10/161], Loss: 0.7658\n",
            "Epoch [42/50], Step [20/161], Loss: 0.8965\n",
            "Epoch [42/50], Step [30/161], Loss: 0.4187\n",
            "Epoch [42/50], Step [40/161], Loss: 0.5107\n",
            "Epoch [42/50], Step [50/161], Loss: 0.4638\n",
            "Epoch [42/50], Step [60/161], Loss: 0.7598\n",
            "Epoch [42/50], Step [70/161], Loss: 0.5895\n",
            "Epoch [42/50], Step [80/161], Loss: 0.5315\n",
            "Epoch [42/50], Step [90/161], Loss: 0.6297\n",
            "Epoch [42/50], Step [100/161], Loss: 0.4160\n",
            "Epoch [42/50], Step [110/161], Loss: 0.5536\n",
            "Epoch [42/50], Step [120/161], Loss: 0.7419\n",
            "Epoch [42/50], Step [130/161], Loss: 1.0238\n",
            "Epoch [42/50], Step [140/161], Loss: 1.3922\n",
            "Epoch [42/50], Step [150/161], Loss: 0.7647\n",
            "Epoch [42/50], Step [160/161], Loss: 0.5694\n",
            "Train accuracy is: 93.30218068535825 %\n",
            "validation accuracy is: 100.0 %\n",
            "Epoch [43/50], Step [10/161], Loss: 0.6663\n",
            "Epoch [43/50], Step [20/161], Loss: 0.4291\n",
            "Epoch [43/50], Step [30/161], Loss: 0.6809\n",
            "Epoch [43/50], Step [40/161], Loss: 0.5600\n",
            "Epoch [43/50], Step [50/161], Loss: 0.1767\n",
            "Epoch [43/50], Step [60/161], Loss: 0.4333\n",
            "Epoch [43/50], Step [70/161], Loss: 0.5956\n",
            "Epoch [43/50], Step [80/161], Loss: 0.4693\n",
            "Epoch [43/50], Step [90/161], Loss: 0.4765\n",
            "Epoch [43/50], Step [100/161], Loss: 0.5439\n",
            "Epoch [43/50], Step [110/161], Loss: 0.7337\n",
            "Epoch [43/50], Step [120/161], Loss: 0.4830\n",
            "Epoch [43/50], Step [130/161], Loss: 0.1100\n",
            "Epoch [43/50], Step [140/161], Loss: 0.5881\n",
            "Epoch [43/50], Step [150/161], Loss: 1.1368\n",
            "Epoch [43/50], Step [160/161], Loss: 1.1169\n",
            "Train accuracy is: 92.21183800623054 %\n",
            "validation accuracy is: 100.0 %\n",
            "Epoch [44/50], Step [10/161], Loss: 0.9407\n",
            "Epoch [44/50], Step [20/161], Loss: 0.6273\n",
            "Epoch [44/50], Step [30/161], Loss: 0.5281\n",
            "Epoch [44/50], Step [40/161], Loss: 1.2890\n",
            "Epoch [44/50], Step [50/161], Loss: 0.5977\n",
            "Epoch [44/50], Step [60/161], Loss: 0.4972\n",
            "Epoch [44/50], Step [70/161], Loss: 0.8785\n",
            "Epoch [44/50], Step [80/161], Loss: 0.7841\n",
            "Epoch [44/50], Step [90/161], Loss: 0.4265\n",
            "Epoch [44/50], Step [100/161], Loss: 0.6167\n",
            "Epoch [44/50], Step [110/161], Loss: 0.7129\n",
            "Epoch [44/50], Step [120/161], Loss: 0.7023\n",
            "Epoch [44/50], Step [130/161], Loss: 0.6940\n",
            "Epoch [44/50], Step [140/161], Loss: 0.5395\n",
            "Epoch [44/50], Step [150/161], Loss: 0.6054\n",
            "Epoch [44/50], Step [160/161], Loss: 0.8651\n",
            "Train accuracy is: 92.83489096573209 %\n",
            "validation accuracy is: 100.0 %\n",
            "Epoch [45/50], Step [10/161], Loss: 0.5584\n",
            "Epoch [45/50], Step [20/161], Loss: 0.7039\n",
            "Epoch [45/50], Step [30/161], Loss: 0.9882\n",
            "Epoch [45/50], Step [40/161], Loss: 1.0546\n",
            "Epoch [45/50], Step [50/161], Loss: 1.2527\n",
            "Epoch [45/50], Step [60/161], Loss: 0.6699\n",
            "Epoch [45/50], Step [70/161], Loss: 0.5988\n",
            "Epoch [45/50], Step [80/161], Loss: 0.8971\n",
            "Epoch [45/50], Step [90/161], Loss: 1.7689\n",
            "Epoch [45/50], Step [100/161], Loss: 0.6248\n",
            "Epoch [45/50], Step [110/161], Loss: 0.5409\n",
            "Epoch [45/50], Step [120/161], Loss: 0.1014\n",
            "Epoch [45/50], Step [130/161], Loss: 0.4355\n",
            "Epoch [45/50], Step [140/161], Loss: 0.5674\n",
            "Epoch [45/50], Step [150/161], Loss: 1.8305\n",
            "Epoch [45/50], Step [160/161], Loss: 0.6475\n",
            "Train accuracy is: 92.6791277258567 %\n",
            "validation accuracy is: 99.30555555555556 %\n",
            "Epoch [46/50], Step [10/161], Loss: 0.8511\n",
            "Epoch [46/50], Step [20/161], Loss: 0.4431\n",
            "Epoch [46/50], Step [30/161], Loss: 0.5687\n",
            "Epoch [46/50], Step [40/161], Loss: 0.5705\n",
            "Epoch [46/50], Step [50/161], Loss: 1.1302\n",
            "Epoch [46/50], Step [60/161], Loss: 0.5560\n",
            "Epoch [46/50], Step [70/161], Loss: 0.4418\n",
            "Epoch [46/50], Step [80/161], Loss: 1.2215\n",
            "Epoch [46/50], Step [90/161], Loss: 0.7739\n",
            "Epoch [46/50], Step [100/161], Loss: 0.1280\n",
            "Epoch [46/50], Step [110/161], Loss: 0.5784\n",
            "Epoch [46/50], Step [120/161], Loss: 0.7380\n",
            "Epoch [46/50], Step [130/161], Loss: 0.9545\n",
            "Epoch [46/50], Step [140/161], Loss: 1.9308\n",
            "Epoch [46/50], Step [150/161], Loss: 0.5127\n",
            "Epoch [46/50], Step [160/161], Loss: 0.6697\n",
            "Train accuracy is: 90.88785046728972 %\n",
            "validation accuracy is: 99.30555555555556 %\n",
            "Epoch [47/50], Step [10/161], Loss: 0.6328\n",
            "Epoch [47/50], Step [20/161], Loss: 0.5906\n",
            "Epoch [47/50], Step [30/161], Loss: 0.5805\n",
            "Epoch [47/50], Step [40/161], Loss: 1.1332\n",
            "Epoch [47/50], Step [50/161], Loss: 0.4252\n",
            "Epoch [47/50], Step [60/161], Loss: 1.0838\n",
            "Epoch [47/50], Step [70/161], Loss: 0.5701\n",
            "Epoch [47/50], Step [80/161], Loss: 0.6447\n",
            "Epoch [47/50], Step [90/161], Loss: 0.4803\n",
            "Epoch [47/50], Step [100/161], Loss: 0.7843\n",
            "Epoch [47/50], Step [110/161], Loss: 0.6384\n",
            "Epoch [47/50], Step [120/161], Loss: 0.6052\n",
            "Epoch [47/50], Step [130/161], Loss: 0.4204\n",
            "Epoch [47/50], Step [140/161], Loss: 0.6633\n",
            "Epoch [47/50], Step [150/161], Loss: 0.5955\n",
            "Epoch [47/50], Step [160/161], Loss: 0.6237\n",
            "Train accuracy is: 92.6791277258567 %\n",
            "validation accuracy is: 100.0 %\n",
            "Epoch [48/50], Step [10/161], Loss: 0.6062\n",
            "Epoch [48/50], Step [20/161], Loss: 0.5863\n",
            "Epoch [48/50], Step [30/161], Loss: 1.1291\n",
            "Epoch [48/50], Step [40/161], Loss: 0.5603\n",
            "Epoch [48/50], Step [50/161], Loss: 0.5247\n",
            "Epoch [48/50], Step [60/161], Loss: 0.5590\n",
            "Epoch [48/50], Step [70/161], Loss: 1.0397\n",
            "Epoch [48/50], Step [80/161], Loss: 0.6650\n",
            "Epoch [48/50], Step [90/161], Loss: 0.6829\n",
            "Epoch [48/50], Step [100/161], Loss: 1.0631\n",
            "Epoch [48/50], Step [110/161], Loss: 0.7619\n",
            "Epoch [48/50], Step [120/161], Loss: 0.5179\n",
            "Epoch [48/50], Step [130/161], Loss: 0.9831\n",
            "Epoch [48/50], Step [140/161], Loss: 0.4656\n",
            "Epoch [48/50], Step [150/161], Loss: 0.5008\n",
            "Epoch [48/50], Step [160/161], Loss: 1.0841\n",
            "Train accuracy is: 92.28971962616822 %\n",
            "validation accuracy is: 100.0 %\n",
            "Epoch [49/50], Step [10/161], Loss: 0.6578\n",
            "Epoch [49/50], Step [20/161], Loss: 0.5665\n",
            "Epoch [49/50], Step [30/161], Loss: 0.8788\n",
            "Epoch [49/50], Step [40/161], Loss: 1.3252\n",
            "Epoch [49/50], Step [50/161], Loss: 0.5398\n",
            "Epoch [49/50], Step [60/161], Loss: 0.5377\n",
            "Epoch [49/50], Step [70/161], Loss: 0.6545\n",
            "Epoch [49/50], Step [80/161], Loss: 1.0649\n",
            "Epoch [49/50], Step [90/161], Loss: 0.4469\n",
            "Epoch [49/50], Step [100/161], Loss: 1.4824\n",
            "Epoch [49/50], Step [110/161], Loss: 0.4319\n",
            "Epoch [49/50], Step [120/161], Loss: 1.0270\n",
            "Epoch [49/50], Step [130/161], Loss: 0.5506\n",
            "Epoch [49/50], Step [140/161], Loss: 0.5606\n",
            "Epoch [49/50], Step [150/161], Loss: 1.2963\n",
            "Epoch [49/50], Step [160/161], Loss: 0.7162\n",
            "Train accuracy is: 92.36760124610592 %\n",
            "validation accuracy is: 100.0 %\n",
            "Epoch [50/50], Step [10/161], Loss: 0.6679\n",
            "Epoch [50/50], Step [20/161], Loss: 0.4886\n",
            "Epoch [50/50], Step [30/161], Loss: 0.4484\n",
            "Epoch [50/50], Step [40/161], Loss: 0.4207\n",
            "Epoch [50/50], Step [50/161], Loss: 0.6059\n",
            "Epoch [50/50], Step [60/161], Loss: 0.6098\n",
            "Epoch [50/50], Step [70/161], Loss: 0.5607\n",
            "Epoch [50/50], Step [80/161], Loss: 0.5169\n",
            "Epoch [50/50], Step [90/161], Loss: 0.7204\n",
            "Epoch [50/50], Step [100/161], Loss: 0.4586\n",
            "Epoch [50/50], Step [110/161], Loss: 0.4414\n",
            "Epoch [50/50], Step [120/161], Loss: 0.5499\n",
            "Epoch [50/50], Step [130/161], Loss: 0.5656\n",
            "Epoch [50/50], Step [140/161], Loss: 0.5658\n",
            "Epoch [50/50], Step [150/161], Loss: 0.7037\n",
            "Epoch [50/50], Step [160/161], Loss: 0.6469\n",
            "Train accuracy is: 94.47040498442368 %\n",
            "validation accuracy is: 100.0 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "loss_history = []\n",
        "val_loss_history = []\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "# Initialize the model for this run\n",
        "fine_tune = True\n",
        "pretrained = True\n",
        "model = ResNet50(num_classes, fine_tune, pretrained).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "contrastive_loss = ContrastiveLoss()\n",
        "distance = torch.nn.MSELoss()\n",
        "# Train the model\n",
        "lr = learning_rate\n",
        "total_step = len(lyme_train_data_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    correct_image_one = 0\n",
        "    correct_image_two = 0\n",
        "    total = 0 \n",
        "    for batch_idx, data in enumerate(lyme_train_data_loader):\n",
        "        [pair_image_one, pair_image_two], get_pair_label, [label_image_one, label_image_two] = data\n",
        "        # Move tensors to the configured device\n",
        "        pair_image_one = pair_image_one.to(device)\n",
        "        pair_image_two = pair_image_two.to(device)\n",
        "\n",
        "        label_image_one = label_image_one.to(device)\n",
        "        label_image_two = label_image_two.to(device)\n",
        "        get_pair_label = get_pair_label.to(device)\n",
        "        \n",
        "        # Forward pass of positive image\n",
        "        features_image_one, output_one = model(pair_image_one)\n",
        "\n",
        "        features_image_one, output_one = features_image_one.to(device), output_one.to(device)\n",
        "        # Forward pass of negative image\n",
        "        features_image_two, output_two = model(pair_image_two)\n",
        "        features_image_two, output_two = features_image_two.to(device), output_two.to(device)\n",
        "\n",
        "        predicted_one = torch.where(torch.sigmoid(output_one.data) > 0.5, 1, 0)\n",
        "        predicted_two = torch.where(torch.sigmoid(output_two.data) > 0.5, 1, 0)\n",
        "\n",
        "        # compute loss for positive image\n",
        "        loss = criterion(output_one, label_image_one.float()) \n",
        "        # compute loss for negative image \n",
        "        loss += criterion(output_two, label_image_two.float()) \n",
        "        # normalize\n",
        "        loss *= 0.5\n",
        "        \n",
        "        distance_features = distance(features_image_one, features_image_two)\n",
        "        loss_features_similarity = contrastive_loss(distance_features, get_pair_label)\n",
        "        \n",
        "        loss += loss_features_similarity\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch_idx+1) % 10 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, batch_idx+1, total_step, loss.item()))\n",
        "        total += label_image_one.size(0)\n",
        "        correct_image_one += (predicted_one == label_image_one).sum().item()\n",
        "        \n",
        "        total += label_image_two.size(0)\n",
        "        correct_image_two += (predicted_two == label_image_two).sum().item()\n",
        "        \n",
        "        correct = (correct_image_one + correct_image_two)\n",
        "        \n",
        "        \n",
        "    print('Train accuracy is: {} %'.format(100 * correct / total))\n",
        "    train_acc_history.append(100 * correct / total)\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    with (torch.no_grad()):\n",
        "      total = 0\n",
        "      correct = 0\n",
        "      model.eval()\n",
        "      for batch_idx, data in enumerate(lyme_validation_data_loader):\n",
        "          [pair_image_one, pair_image_two], get_pair_label, [label_image_one, label_image_two] = data\n",
        "          # Move tensors to the configured device\n",
        "          pair_image_one = pair_image_one.to(device)\n",
        "          pair_image_two = pair_image_two.to(device)\n",
        "\n",
        "          label_image_one = label_image_one.to(device)\n",
        "          label_image_two = label_image_two.to(device)\n",
        "          # Forward pass of positive image\n",
        "          features_image_one, output_one = model(pair_image_one)\n",
        "          features_image_two, output_two = model(pair_image_two)\n",
        "\n",
        "          features_image_one, output_one = features_image_one.to(device), output_one.to(device)\n",
        "          features_image_two, output_two = features_image_two.to(device), output_two.to(device)\n",
        "\n",
        "          predicted_one = torch.where(torch.sigmoid(output_one.data) > 0.5, 1, 0)\n",
        "          predicted_two = torch.where(torch.sigmoid(output_two.data) > 0.5, 1, 0)\n",
        "\n",
        "          total += label_image_one.size(0)\n",
        "          total += label_image_two.size(0)\n",
        "          correct += (predicted_one == label_image_one).sum().item()\n",
        "          correct += (predicted_two == label_image_two).sum().item()\n",
        "          \n",
        "\n",
        "                    \n",
        "      print('validation accuracy is: {} %'.format(100 * correct / total))\n",
        "      val_acc_history.append(100 * correct / total)\n",
        "\n",
        "#TODO: add validation\n",
        "\n",
        "plt.plot(train_acc_history)\n",
        "plt.plot(val_acc_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy history')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# plot the loss history\n",
        "plt.plot(loss_history)\n",
        "plt.plot(val_loss_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss history')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_cbgf57NsxSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee6dcdd-0f84-443b-9aff-a1a02005426b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy is: 93.10344827586206 %\n"
          ]
        }
      ],
      "source": [
        "with (torch.no_grad()):\n",
        "      total = 0\n",
        "      correct = 0\n",
        "      model.eval()\n",
        "      for batch_idx, data in enumerate(lyme_test_data_loader):\n",
        "          pair_image_one, label_image_one = data\n",
        "          # Move tensors to the configured device\n",
        "          pair_image_one = pair_image_one.to(device)\n",
        "          #pair_image_two = pair_image_two.to(device)\n",
        "\n",
        "          label_image_one = label_image_one.to(device)\n",
        "          #label_image_two = label_image_two.to(device)\n",
        "          # Forward pass of positive image\n",
        "          features_image_one, output_one = model(pair_image_one)\n",
        "          #features_image_two, output_two = model(pair_image_two)\n",
        "\n",
        "          features_image_one, output_one = features_image_one.to(device), output_one.to(device)\n",
        "          #features_image_two, output_two = features_image_two.to(device), output_two.to(device)\n",
        "\n",
        "          predicted_one = torch.where(torch.sigmoid(output_one.data) > 0.5, 1, 0)\n",
        "          #predicted_two = torch.where(torch.sigmoid(output_two.data) > 0.5, 1, 0)\n",
        "\n",
        "          total += label_image_one.size(0)\n",
        "          #total += label_image_two.size(0)\n",
        "          correct += (predicted_one == label_image_one).sum().item()\n",
        "          #correct += (predicted_two == label_image_two).sum().item()\n",
        "          \n",
        "\n",
        "                    \n",
        "      print('test accuracy is: {} %'.format(100 * correct / total))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "siamesenotebuk.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb96647fd6cb4148820352c882d10a64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14ec033041024b8891e881fdb85d0ae8",
              "IPY_MODEL_b8027a74060c4b20a3300f7ada44763e",
              "IPY_MODEL_b05a5fa7604341eaa703bc6b6cda0b91"
            ],
            "layout": "IPY_MODEL_350c3cc38fc44414bece91d1acba25f0"
          }
        },
        "14ec033041024b8891e881fdb85d0ae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49d6a16ea5f4405e8d52b946ab53aac2",
            "placeholder": "​",
            "style": "IPY_MODEL_385928c486184664b5e24fea9295abf3",
            "value": "100%"
          }
        },
        "b8027a74060c4b20a3300f7ada44763e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad734b5d64f94ac5ac9f127db7378587",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aec230cbe9db44c0b99acd03c3c98cd2",
            "value": 102530333
          }
        },
        "b05a5fa7604341eaa703bc6b6cda0b91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b08fc6dab2fc409189b50e545dc5e50c",
            "placeholder": "​",
            "style": "IPY_MODEL_ab4a8cfa4a7d4907b3935af9d68ea419",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 125MB/s]"
          }
        },
        "350c3cc38fc44414bece91d1acba25f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49d6a16ea5f4405e8d52b946ab53aac2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "385928c486184664b5e24fea9295abf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad734b5d64f94ac5ac9f127db7378587": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aec230cbe9db44c0b99acd03c3c98cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b08fc6dab2fc409189b50e545dc5e50c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab4a8cfa4a7d4907b3935af9d68ea419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}