{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLCV Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62OCVFrnRZGx"
   },
   "outputs": [],
   "source": [
    "Name = \"Luisa Danalachi\"\n",
    "Matriculation_Number = \"7022909\"\n",
    "\n",
    "Name = \"Victor Martinez Palomares\"\n",
    "Matriculation_Number = \"7021729\"\n",
    "\n",
    "Name = \"Soham Roy\"\n",
    "Matriculation_Number = \"7028704\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "i6fBKi5dRdVQ",
    "outputId": "0010a4e7-2787-484b-bf7b-63d91a2469e7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# !mkdir ./datasets\n",
    "# !mkdir ./datasets/lyme_dataset\n",
    "# !cp -r drive/MyDrive/Colab\\ Notebooks/HLCV_project/datasets/lyme_dataset.zip ./datasets\n",
    "# !unzip -q -o \"./datasets/lyme_dataset.zip\" -d \"./datasets/lyme_dataset\"\n",
    "\n",
    "# '''\n",
    "# !mkdir ./resources\n",
    "\n",
    "# !cp -r drive/MyDrive/Colab\\ Notebooks/HLCV/Exercise_3/resources/fig1.png ./resources\n",
    "# !cp -r drive/MyDrive/Colab\\ Notebooks/HLCV/Exercise_3/resources/fig2.png ./resources\n",
    "\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nifGyyAjRiJd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRgAQ3OtKLeE"
   },
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GTnqkXkIKKs8",
    "outputId": "13884d41-5022-45c1-a3c8-803fef14c658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 256\n",
    "EPOCHS = 120\n",
    "BATCH = 4\n",
    "LR = 1e-4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: %s'%device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define data augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "vc02dP-sWBLl",
    "outputId": "6df3bd1b-ad0a-4725-c651-6a78f33bb395"
   },
   "outputs": [],
   "source": [
    "data_aug_transforms = []\n",
    "\n",
    "data_aug_transforms.append(transforms.RandomRotation([-90, 90]) ) \n",
    "data_aug_transforms.append( transforms.RandomHorizontalFlip() )\n",
    "data_aug_transforms.append(transforms.ColorJitter(brightness = 0.2)) \n",
    "\n",
    "norm_transforms = [transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                   transforms.ToTensor(), \n",
    "                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "\n",
    "train_transforms = transforms.Compose(data_aug_transforms + norm_transforms)\n",
    "\n",
    "# Add Compose\n",
    "test_transforms =transforms.Compose(norm_transforms) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Lyme DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyme Dermnet data: Dataset ImageFolder\n",
      "    Number of datapoints: 357\n",
      "    Root location: ./datasets/Lyme/Train/Train_2_Cases\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomRotation(degrees=[-90.0, 90.0], interpolation=nearest, expand=False, fill=0)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ColorJitter(brightness=[0.8, 1.2], contrast=None, saturation=None, hue=None)\n",
      "               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "87\n",
      "Lyme classes: ['Lyme_Negative', 'Lyme_Positive']\n"
     ]
    }
   ],
   "source": [
    "#Load Lyme\n",
    "lyme_train_data_path = \"./datasets/Lyme/Train/Train_2_Cases\"\n",
    "lyme_test_data_path = \"./datasets/Lyme/Validation/Validation_2_Cases\"\n",
    "\n",
    "# ImageFolder is a generic data loader where the images are arranged in multiple folders\n",
    "\n",
    "#Load TRAIN\n",
    "lyme_train_data = torchvision.datasets.ImageFolder(root=lyme_train_data_path, transform=train_transforms)\n",
    "lyme_train_data_loader = data.DataLoader(lyme_train_data, batch_size=BATCH, shuffle=True)\n",
    "print(\"Lyme Dermnet data:\", lyme_train_data)\n",
    "\n",
    "# Load TEST \n",
    "lyme_test_data = torchvision.datasets.ImageFolder(root=lyme_test_data_path, transform=test_transforms)\n",
    "lyme_test_data_loader = data.DataLoader(lyme_test_data, batch_size=BATCH)\n",
    "print(len(lyme_test_data))\n",
    "# Check list of classes\n",
    "# Are converted to 0,1 etc when call enumerate()\n",
    "list_of_classes=list(map(str, list(lyme_train_data.classes)) )\n",
    "print(\"Lyme classes:\", list_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n",
      "36\n",
      "87\n",
      "80\n",
      "9\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "val_split = 0.1\n",
    "\n",
    "num_training = int((1 - val_split) * len(lyme_train_data))\n",
    "num_validation = len(lyme_train_data) - num_training\n",
    "mask = list(range(num_training))\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(lyme_train_data, mask)\n",
    "mask = list(range(num_training, num_training + num_validation))\n",
    "val_dataset = torch.utils.data.Subset(lyme_train_data, mask)\n",
    "\n",
    "# Create DataLoaders\n",
    "lyme_train_data_loader = data.DataLoader(train_dataset, batch_size=BATCH, shuffle=True, drop_last=True)\n",
    "lyme_validation_data_loader = data.DataLoader(val_dataset, batch_size=BATCH, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(lyme_test_data))\n",
    "\n",
    "print(len(lyme_train_data_loader))\n",
    "print(len(lyme_validation_data_loader))\n",
    "print(len(lyme_test_data_loader)) #did not add drop_last=True to TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator=iter(lyme_validation_data_loader)\n",
    "# inputs, classes = next(iterator)\n",
    "# print(len(inputs)) \n",
    "\n",
    "# plt.imshow(inputs[0].squeeze().permute(2,1,0))\n",
    "# plt.show()\n",
    "# print(\"CLass: \",classes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dermnet DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details Dermnet data: Dataset ImageFolder\n",
      "    Number of datapoints: 15557\n",
      "    Root location: ./datasets/Dermnet/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomRotation(degrees=[-90.0, 90.0], interpolation=nearest, expand=False, fill=0)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ColorJitter(brightness=[0.8, 1.2], contrast=None, saturation=None, hue=None)\n",
      "               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "Dermnet classes ['Acne and Rosacea Photos', 'Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions', 'Atopic Dermatitis Photos', 'Bullous Disease Photos', 'Cellulitis Impetigo and other Bacterial Infections', 'Eczema Photos', 'Exanthems and Drug Eruptions', 'Hair Loss Photos Alopecia and other Hair Diseases', 'Herpes HPV and other STDs Photos', 'Light Diseases and Disorders of Pigmentation', 'Lupus and other Connective Tissue diseases', 'Melanoma Skin Cancer Nevi and Moles', 'Nail Fungus and other Nail Disease', 'Poison Ivy Photos and other Contact Dermatitis', 'Psoriasis pictures Lichen Planus and related diseases', 'Scabies Lyme Disease and other Infestations and Bites', 'Seborrheic Keratoses and other Benign Tumors', 'Systemic Disease', 'Tinea Ringworm Candidiasis and other Fungal Infections', 'Urticaria Hives', 'Vascular Tumors', 'Vasculitis Photos', 'Warts Molluscum and other Viral Infections']\n"
     ]
    }
   ],
   "source": [
    "#Load Dermnet\n",
    "dermnet_train_data_path = \"./datasets/Dermnet/train\"\n",
    "dermnet_test_data_path = \"./datasets/Dermnet/test\"\n",
    "\n",
    "# ImageFolder is a generic data loader where the images are arranged in multiple folders\n",
    "dermnet_train_data = torchvision.datasets.ImageFolder(root=dermnet_train_data_path, transform=train_transforms)\n",
    "dermnet_train_data_loader = data.DataLoader(dermnet_train_data, batch_size=BATCH, shuffle=True)\n",
    "print(\"Details Dermnet data:\", dermnet_train_data)\n",
    "\n",
    "# Load test \n",
    "dermnet_test_data = torchvision.datasets.ImageFolder(root=dermnet_test_data_path, transform=test_transforms)\n",
    "dermnet_test_data_loader = data.DataLoader(dermnet_train_data, batch_size=BATCH)\n",
    "\n",
    "# Load list of classes\n",
    "list_of_classes=list(map(str, list(dermnet_train_data.classes)) )\n",
    "print(\"Dermnet classes\", list_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load HAM DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id   dx dx_type   age   sex localization\n",
       "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n",
       "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n",
       "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n",
       "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n",
       "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_df = pd.read_csv('./datasets/HAM10000/HAM10000_metadata.csv')\n",
    "ham_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract images based on their label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take labels\n",
    "name_labels = ham_df[\"dx\"].unique()\n",
    "\n",
    "# Create folders with the label name and add the corresponding images -> Must for using ImageFolder!\n",
    "\n",
    "for label in range(len(name_labels)):\n",
    "    # create folder for each label\n",
    "    label_folder_path = \"./datasets/HAM10000/train/\" + str(name_labels[label])\n",
    "    \n",
    "    # check is path exists if not create folder\n",
    "    if not os.path.exists(label_folder_path):\n",
    "        os.mkdir('./datasets/HAM10000/train/' + name_labels[label] + \"/\" )\n",
    "    \n",
    "    # take the image id corresponding to label\n",
    "    image_names =  ham_df[ham_df['dx'] == name_labels[label]]['image_id']\n",
    "    \n",
    "    # iterate through all image names \n",
    "    for image in image_names:\n",
    "        # create the path for image: either part 1 or part 2\n",
    "        path_folder_1 = \"./datasets/HAM10000/HAM10000_images_part_1/\" + image + \".jpg\"\n",
    "        path_folder_2 = \"./datasets/HAM10000/HAM10000_images_part_2/\" + image + \".jpg\"\n",
    "        \n",
    "        # find where is the image and copy it into the label folder\n",
    "        if os.path.exists(path_folder_1):\n",
    "            shutil.copyfile(path_folder_1, './datasets/HAM10000/train/' + name_labels[label] + \"/\" + image + \".jpg\")\n",
    "        else:\n",
    "            shutil.copyfile(path_folder_2, './datasets/HAM10000/train/' + name_labels[label] + \"/\" + image + \".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAM1000 Dermnet data: Dataset ImageFolder\n",
      "    Number of datapoints: 10015\n",
      "    Root location: ./datasets/HAM10000/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomRotation(degrees=[-90.0, 90.0], interpolation=nearest, expand=False, fill=0)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ColorJitter(brightness=[0.8, 1.2], contrast=None, saturation=None, hue=None)\n",
      "               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "HAM1000 classes ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n"
     ]
    }
   ],
   "source": [
    "#Load Dermnet\n",
    "HAM_train_data_path = \"./datasets/HAM10000/train\"\n",
    "\n",
    "# ImageFolder is a generic data loader where the images are arranged in multiple folders\n",
    "ham_train_data = torchvision.datasets.ImageFolder(root=HAM_train_data_path, transform=train_transforms)\n",
    "ham_train_data_loader = data.DataLoader(ham_train_data, batch_size=BATCH, shuffle=True)\n",
    "print(\"HAM1000 Dermnet data:\", ham_train_data)\n",
    "\n",
    "# # Load list of classes\n",
    "list_of_classes=list(map(str, list(ham_train_data.classes)) )\n",
    "print(\"HAM1000 classes\", list_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSDvF5JPQbDS"
   },
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ct4o47mpQeMc",
    "outputId": "bc098a58-fc09-4300-dd4e-c1405d4aefa0"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "#from torchvision.models import ResNet50_Weights\n",
    "\n",
    "layer_config= [2048, 512, 256]\n",
    "num_classes = 2\n",
    "num_epochs = 30\n",
    "batch_size = 200\n",
    "learning_rate = 1e-5\n",
    "learning_rate_decay = 0.99\n",
    "\n",
    "# Create ResNet 50 model pretrained with ImageNet\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, n_class, fine_tune, pretrained=True):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.net = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        # add new classifier layers\n",
    "        self.net.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(layer_config[0], layer_config[1]),\n",
    "            nn.BatchNorm1d(layer_config[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_config[1], layer_config[2]),\n",
    "            nn.BatchNorm1d(layer_config[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_config[2], n_class)\n",
    "        )       \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Initialize the model for this run\n",
    "fine_tune = True\n",
    "pretrained = True\n",
    "model= ResNet50(num_classes, fine_tune, pretrained)\n",
    "print(model)\n",
    "\n",
    "\n",
    "print(\"Params to learn:\")\n",
    "if fine_tune:\n",
    "    params_to_update = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    params_to_update = model.parameters()\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-FabsTwYqzA",
    "outputId": "ab944b36-65e8-419a-8bd3-2c8a33ad789e"
   },
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=\"1\"\n",
    "\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "# store best model and acc\n",
    "best_model_name = 'bestmodel_resnet50_imagenet.ckpt'\n",
    "best_model = None\n",
    "best_val_acc = 0.\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=learning_rate)\n",
    "# Train the model\n",
    "lr = learning_rate\n",
    "total_step = len(lyme_train_data_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0 \n",
    "    for i, (images, labels) in enumerate(lyme_train_data_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images).to(device)\n",
    "        _, predicted = torch.max(outputs.data, 1)    \n",
    "        loss = criterion(outputs, labels.float()) # labels are stored as float need cast to int\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Train accuracy is: {} %'.format(100 * correct / total))\n",
    "    train_acc_history.append(100 * correct / total)\n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    # Code to update the lr\n",
    "    lr *= learning_rate_decay\n",
    "    update_lr(optimizer, lr)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in lyme_validation_data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images).to(device)\n",
    "            _, predicted = torch.max(outputs.data, 1)    \n",
    "            \n",
    "            loss = criterion(outputs, labels.float())\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "        val_accuracy = 100 * correct / total\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            best_model = model\n",
    "            print(\"New best validation accuracy: {} %\".format(best_val_acc))\n",
    "\n",
    "\n",
    "        print('Validataion accuracy is: {} %'.format(100 * correct / total))\n",
    "        val_acc_history.append(val_accuracy)\n",
    "        val_loss_history.append(loss.item())\n",
    "  \n",
    "plt.plot(train_acc_history)\n",
    "plt.plot(val_acc_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy history')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(loss_history)\n",
    "plt.plot(val_loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss history')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn-lyme-detection.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
